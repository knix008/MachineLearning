import torch
import gradio as gr
import time
from diffusers import FluxKontextPipeline
from PIL import Image
import datetime


def loading_model():
    model_id = "black-forest-labs/FLUX.1-Kontext-dev"
    print("ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
    pipe = FluxKontextPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)

    # CPU ì˜¤í”„ë¡œë“œ ë° Attention ìŠ¬ë¼ì´ì‹± í™œì„±í™”
    pipe.enable_model_cpu_offload()
    pipe.enable_sequential_cpu_offload()
    pipe.enable_attention_slicing(1)
    pipe.enable_vae_slicing()
    print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    return pipe

# ë¡œë”©ëœ ëª¨ë¸ì„ ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥
pipe = loading_model()


def resize_image(input_image):
    # ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°
    w, h = input_image.size
    print(f"ì›ë³¸ ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: {w}x{h}")

    # 16ìœ¼ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ë„ë¡ í¬ê¸° ì¡°ì •
    def round_to_multiple(value, multiple=16):
        return ((value + multiple // 2) // multiple) * multiple

    # ì›ë³¸ ë¹„ìœ¨ ê³„ì‚°
    aspect_ratio = w / h
    
    # ìµœëŒ€ í¬ê¸° ì œí•œ (1360)
    max_size = 1360
    
    # ë” í° ìª½ì´ 1360ì„ ë„˜ëŠ” ê²½ìš° ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©° ì¶•ì†Œ
    if max(w, h) > max_size:
        if w >= h:
            # ê°€ë¡œê°€ ë” í° ê²½ìš°
            scale_factor = max_size / w
            w = max_size
            h = int(h * scale_factor)
        else:
            # ì„¸ë¡œê°€ ë” í° ê²½ìš°
            scale_factor = max_size / h
            h = max_size
            w = int(w * scale_factor)
        print(f"ìµœëŒ€ í¬ê¸° ì œí•œ ì ìš© í›„: {w}x{h}")

    # 16ìœ¼ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ë„ë¡ í¬ê¸° ì¡°ì •
    if w >= h:
        # ê°€ë¡œê°€ ë” í¬ê±°ë‚˜ ê°™ì€ ê²½ìš°
        new_w = round_to_multiple(w)
        new_h = round_to_multiple(int(new_w / aspect_ratio))
        
        # ìƒˆë¡œìš´ í¬ê¸°ê°€ ì œí•œì„ ë„˜ëŠ” ê²½ìš° ë‹¤ì‹œ ì¡°ì •
        if new_w > max_size:
            new_w = round_to_multiple(max_size - 16)  # í•œ ë‹¨ê³„ ì‘ê²Œ
            new_h = round_to_multiple(int(new_w / aspect_ratio))
    else:
        # ì„¸ë¡œê°€ ë” í° ê²½ìš°
        new_h = round_to_multiple(h)
        new_w = round_to_multiple(int(new_h * aspect_ratio))
        
        # ìƒˆë¡œìš´ í¬ê¸°ê°€ ì œí•œì„ ë„˜ëŠ” ê²½ìš° ë‹¤ì‹œ ì¡°ì •
        if new_h > max_size:
            new_h = round_to_multiple(max_size - 16)  # í•œ ë‹¨ê³„ ì‘ê²Œ
            new_w = round_to_multiple(int(new_h * aspect_ratio))

    # ë¹„ìœ¨ì´ ë§ì´ í‹€ì–´ì§€ì§€ ì•Šë„ë¡ ì¬ì¡°ì •
    actual_ratio = new_w / new_h
    if abs(actual_ratio - aspect_ratio) > 0.1:  # ë¹„ìœ¨ ì°¨ì´ê°€ 10% ì´ìƒì¸ ê²½ìš°
        if aspect_ratio > actual_ratio:
            new_w = min(round_to_multiple(int(new_h * aspect_ratio)), max_size)
        else:
            new_h = min(round_to_multiple(int(new_w / aspect_ratio)), max_size)

    print(f"ì¡°ì •ëœ ì´ë¯¸ì§€ í¬ê¸°: {new_w}x{new_h}")
    print(f"ì›ë³¸ ë¹„ìœ¨: {aspect_ratio:.3f}, ì¡°ì •ëœ ë¹„ìœ¨: {new_w/new_h:.3f}")

    # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ê³ í’ˆì§ˆ ë¦¬ìƒ˜í”Œë§ ì‚¬ìš©)
    resized_image = input_image.resize((new_w, new_h), Image.Resampling.LANCZOS)

    return resized_image


def generate_image(
    prompt,
    negative_prompt,
    input_image,
    guidance_scale,
    steps,
    seq_len,
    seed,
):
    start = time.time()

    if input_image is None:
        return None, "ì´ë¯¸ì§€-íˆ¬-ì´ë¯¸ì§€ë§Œ ì§€ì›í•©ë‹ˆë‹¤. ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”."

    input_image = resize_image(input_image)  # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
    info = f"\nì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: {input_image.size[0]}x{input_image.size[1]}"
    # Generator ì„¤ì •
    if seed == -1:
        seed = torch.randint(0, 2**32 - 1, (1,)).item()
    generator = torch.Generator("cpu").manual_seed(seed)

    try:
        image = pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            image=input_image,
            width=input_image.size[0],
            height=input_image.size[1],
            guidance_scale=guidance_scale,
            num_inference_steps=int(steps),
            max_sequence_length=int(seq_len),
            generator=generator,
        ).images[0]

        # ê³ í’ˆì§ˆ ì €ì¥ ì„¤ì •
        filename = f"flux1-kontext-dev-example13_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}.png"
        image.save(
            filename, format="PNG", compress_level=1, optimize=False
        )  # ìµœê³  í’ˆì§ˆë¡œ ì €ì¥
        info_text = (
            f"ìƒì„± ì™„ë£Œ! (ì´ë¯¸ì§€ íˆ¬ ì´ë¯¸ì§€)\nì‹œê°„: {time.time()-start:.2f}ì´ˆ\nì‹œë“œ: {seed}\nì €ì¥ëœ íŒŒì¼: {filename}"
            f"\nìƒì„±ëœ ì´ë¯¸ì§€ í¬ê¸°: {image.size[0]}x{image.size[1]}{info}"
        )
        return image, info_text
    except Exception as e:
        return None, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
with gr.Blocks(title="FLUX.1 Kontext Dev ì´ë¯¸ì§€ ìƒì„±ê¸°") as demo:
    gr.Markdown("# ğŸ¨ FLUX.1 Kontext Devì´ë¯¸ì§€ ìƒì„±ê¸°")

    with gr.Row():
        with gr.Column():
            # ì…ë ¥ ì´ë¯¸ì§€ (ì„ íƒì‚¬í•­)
            input_image = gr.Image(
                label="ì…ë ¥ ì´ë¯¸ì§€ (ì„ íƒì‚¬í•­)",
                type="pil",
                sources=["upload", "clipboard"],
                height=500,
                value="default.jpg",
            )

            # ì…ë ¥ ì»¨íŠ¸ë¡¤ë“¤
            prompt_input = gr.Textbox(
                label="í”„ë¡¬í”„íŠ¸",
                placeholder="ìƒì„±í•˜ê³  ì‹¶ì€ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”...",
                value="8k, high detail, high quality, realistic, masterpiece, best quality, dark blue bikini",
                lines=4,
            )

            negative_prompt_input = gr.Textbox(
                label="ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸",
                placeholder="ì›í•˜ì§€ ì•ŠëŠ” ìš”ì†Œë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                value="blurring, low quality, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, normal quality, jpeg artifacts, signature, watermark, username",
                lines=2,
            )

            guidance_slider = gr.Slider(
                minimum=1.0,
                maximum=10.0,  # ë²”ìœ„ í™•ì¥
                value=6.5,  # ë” ë†’ì€ ê¸°ë³¸ê°’
                step=0.1,
                label="ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼",
                info="í”„ë¡¬í”„íŠ¸ ì¤€ìˆ˜ ì •ë„. ë†’ì„ìˆ˜ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ë” ì •í™•íˆ ë”°ë¥´ì§€ë§Œ ì°½ì˜ì„±ì´ ì¤„ì–´ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ê¶Œì¥: 7.0-10.0)",
            )

            steps_slider = gr.Slider(
                minimum=10,
                maximum=50,  # ë” ë†’ì€ ìµœëŒ€ê°’
                value=35,  # ë” ë†’ì€ ê¸°ë³¸ê°’
                step=1,
                label="ì¶”ë¡  ìŠ¤í… ìˆ˜",
                info="ì´ë¯¸ì§€ ìƒì„± ë‹¨ê³„ ìˆ˜. ë†’ì„ìˆ˜ë¡ í’ˆì§ˆì´ í–¥ìƒë˜ì§€ë§Œ ìƒì„± ì‹œê°„ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. (ìµœê³  í’ˆì§ˆ: 50-80)",
            )

            sequence_slider = gr.Slider(
                minimum=256,
                maximum=1024,  # ë” ë†’ì€ ìµœëŒ€ê°’
                value=512,  # ë” ë†’ì€ ê¸°ë³¸ê°’
                step=64,
                label="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´",
                info="í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ê¸¸ì´. ê¸´ í”„ë¡¬í”„íŠ¸ì—ëŠ” ë†’ì€ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤. (ìµœê³  í’ˆì§ˆ: 512-1024)",
            )

            seed_input = gr.Number(
                label="ì‹œë“œ (-1ì€ ëœë¤)",
                value=100,
                precision=0,
                info="ìƒì„± ê²°ê³¼ì˜ ì¼ê´€ì„±ì„ ìœ„í•œ ë‚œìˆ˜ ì‹œë“œ. ê°™ì€ ì‹œë“œë¡œ ê°™ì€ ì„¤ì •ì´ë©´ ë¹„ìŠ·í•œ ê²°ê³¼ê°€ ë‚˜ì˜µë‹ˆë‹¤. -1ì€ ë¬´ì‘ìœ„ ì‹œë“œ ì‚¬ìš©",
            )

            generate_btn = gr.Button("ğŸ¨ ì´ë¯¸ì§€ ìƒì„±", variant="primary", size="lg")

        with gr.Column():
            # ì¶œë ¥ ì˜ì—­
            output_image = gr.Image(label="ìƒì„±ëœ ì´ë¯¸ì§€", type="pil", height=500)
            info_output = gr.Textbox(label="ìƒì„± ì •ë³´", lines=4, interactive=False)

    # ì´ë²¤íŠ¸ ì—°ê²°
    generate_btn.click(
        fn=generate_image,
        inputs=[
            prompt_input,
            negative_prompt_input,  # ì¶”ê°€
            input_image,
            guidance_slider,
            steps_slider,
            sequence_slider,
            seed_input,
        ],
        outputs=[output_image, info_output],
    )

if __name__ == "__main__":
    demo.launch(inbrowser=True)
