import gradio as gr
import torch
from diffusers import StableDiffusionImg2ImgPipeline, DPMSolverMultistepScheduler, UNet2DConditionModel, AutoencoderKL
from transformers.models.clip import CLIPTextModel, CLIPTokenizer, CLIPImageProcessor
from PIL import Image
import numpy as np

# Hugging Face Access Token (í•„ìš”ì‹œ ì‚¬ìš©)
#access_token = ""
#from huggingface_hub import login
#login(access_token)


def load_model():
    """Stable Diffusion 3.5 Medium ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    model_id = "stabilityai/stable-diffusion-3.5-medium"

    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    torch_dtype = torch.float16 if device == "cuda" else torch.float32
    
    print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
    print(f"ì‚¬ìš© ì¤‘ì¸ ë°ì´í„° íƒ€ì…: {torch_dtype}")

    # íŒŒì´í”„ë¼ì¸ ìƒì„± - ê¸°ë³¸ ë¡œë”© í›„ ì»´í¬ë„ŒíŠ¸ êµì²´
    try:
        print("ëª¨ë¸ ë¡œë”© ì¤‘...")
        # ë¨¼ì € ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            model_id, 
            torch_dtype=torch_dtype, 
            use_safetensors=True,
            feature_extractor=None,
            image_encoder=None,
            text_encoder=None,
            tokenizer=None,
            scheduler=None,
            safety_checker=None,
            unet=None,
            vae=None,
            requires_safety_checker=False
        )
        print("ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì™„ë£Œ!")
        
        # ê°œë³„ ì»´í¬ë„ŒíŠ¸ êµì²´ ì‹œë„
        try:
            print("UNet êµì²´ ì¤‘...")
            unet = UNet2DConditionModel.from_pretrained(
                model_id, 
                subfolder="unet",
                torch_dtype=torch_dtype,
                use_safetensors=True
            )
            pipe.unet = unet
            print("UNet êµì²´ ì™„ë£Œ!")
        except Exception as e:
            print(f"UNet êµì²´ ì‹¤íŒ¨: {e}")
        
        try:
            print("VAE êµì²´ ì¤‘...")
            vae = AutoencoderKL.from_pretrained(
                model_id,
                subfolder="vae",
                torch_dtype=torch_dtype,
                use_safetensors=True
            )
            pipe.vae = vae
            print("VAE êµì²´ ì™„ë£Œ!")
        except Exception as e:
            print(f"VAE êµì²´ ì‹¤íŒ¨: {e}")
        
        try:
            print("Text Encoder êµì²´ ì¤‘...")
            text_encoder = CLIPTextModel.from_pretrained(
                model_id,
                subfolder="text_encoder",
                torch_dtype=torch_dtype
            )
            pipe.text_encoder = text_encoder
            print("Text Encoder êµì²´ ì™„ë£Œ!")
        except Exception as e:
            print(f"Text Encoder êµì²´ ì‹¤íŒ¨: {e}")
        
        try:
            print("Tokenizer êµì²´ ì¤‘...")
            tokenizer = CLIPTokenizer.from_pretrained(
                model_id,
                subfolder="tokenizer"
            )
            pipe.tokenizer = tokenizer
            print("Tokenizer êµì²´ ì™„ë£Œ!")
        except Exception as e:
            print(f"Tokenizer êµì²´ ì‹¤íŒ¨: {e}")
        
        try:
            print("Feature Extractor êµì²´ ì¤‘...")
            feature_extractor = CLIPImageProcessor.from_pretrained(
                model_id,
                subfolder="feature_extractor"
            )
            pipe.feature_extractor = feature_extractor
            print("Feature Extractor êµì²´ ì™„ë£Œ!")
        except Exception as e:
            print(f"Feature Extractor êµì²´ ì‹¤íŒ¨: {e}")
        
        try:
            print("Image Encoder êµì²´ ì¤‘...")
            image_encoder = CLIPTextModel.from_pretrained(
                model_id,
                subfolder="image_encoder",
                torch_dtype=torch_dtype
            )
            pipe.image_encoder = image_encoder
            print("Image Encoder êµì²´ ì™„ë£Œ!")
        except Exception as e:
            print(f"Image Encoder êµì²´ ì‹¤íŒ¨: {e}")
        
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    except Exception as e:
        print(f"ê¸°ë³¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        print("ìµœì†Œ ì„¤ì •ìœ¼ë¡œ ë¡œë”© ì‹œë„...")
        # ìµœì†Œ ì„¤ì •ìœ¼ë¡œ ë¡œë”©
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            model_id, 
            torch_dtype=torch_dtype
        )

    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    try:
        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
        print("ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ")
    except Exception as e:
        print(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì‹¤íŒ¨, ê¸°ë³¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©: {e}")

    # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    pipe = pipe.to(device)
    
    # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
    try:
        if device == "cuda":
            print("GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            # GPU ë©”ëª¨ë¦¬ ìµœì í™”
            pipe.enable_attention_slicing()
            pipe.enable_vae_slicing()
        else:
            print("CPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            print("CPU ëª¨ë“œì—ì„œëŠ” ìƒì„± ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            # CPU ë©”ëª¨ë¦¬ ìµœì í™”
            pipe.enable_attention_slicing()
            pipe.enable_vae_slicing()
    except Exception as e:
        print(f"ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")

    return pipe


def style_preserving_generation(
    input_image,
    prompt,
    negative_prompt="",
    strength=0.6,
    guidance_scale=7.5,
    num_inference_steps=25,
    seed=-1,
    preserve_style=True,
    style_strength=0.8,
    cpu_optimization=False,
    max_image_size=768,
):
    """
    ì…ë ¥ ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•˜ë©´ì„œ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        input_image: ì…ë ¥ ì´ë¯¸ì§€ (PIL Image)
        prompt: ìƒì„±í•  ì´ë¯¸ì§€ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸
        negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
        strength: ë³€í™˜ ê°•ë„ (0.0-1.0)
        guidance_scale: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
        num_inference_steps: ì¶”ë¡  ìŠ¤í… ìˆ˜
        seed: ëœë¤ ì‹œë“œ (-1ì´ë©´ ëœë¤)
        preserve_style: ìŠ¤íƒ€ì¼ ë³´ì¡´ ì—¬ë¶€
        style_strength: ìŠ¤íƒ€ì¼ ë³´ì¡´ ê°•ë„ (0.0-1.0)

    Returns:
        ìƒì„±ëœ ì´ë¯¸ì§€ì™€ ìƒíƒœ ë©”ì‹œì§€
    """
    try:
        # ëª¨ë¸ ë¡œë“œ
        pipe = load_model()

        # ë””ë°”ì´ìŠ¤ í™•ì¸
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # ì‹œë“œ ì„¤ì •
        if seed == -1:
            seed = torch.randint(0, 2**32, (1,)).item()
            
        generator = torch.Generator(device=device).manual_seed(seed)

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        if input_image is None:
            return None, "ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
        width, height = input_image.size
        if width > max_image_size or height > max_image_size:
            ratio = min(max_image_size / width, max_image_size / height)
            new_width = int(width * ratio)
            new_height = int(height * ratio)
            input_image = input_image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            print(f"ì´ë¯¸ì§€ í¬ê¸°ë¥¼ {width}x{height}ì—ì„œ {new_width}x{new_height}ë¡œ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.")

        # CPU ìµœì í™” ì„¤ì •
        if cpu_optimization and device == "cpu":
            print("CPU ìµœì í™” ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            # CPUì—ì„œ ë” ì•ˆì •ì ì¸ ì„¤ì •
            if num_inference_steps > 25:
                num_inference_steps = 25
                print(f"CPU ìµœì í™”: ì¶”ë¡  ìŠ¤í…ì„ {num_inference_steps}ë¡œ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.")
            if guidance_scale > 8:
                guidance_scale = 8
                print(f"CPU ìµœì í™”: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ì„ {guidance_scale}ë¡œ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.")
            # CPUì—ì„œ ë” ì•ˆì •ì ì¸ strength ì„¤ì •
            if strength > 0.7:
                strength = 0.7
                print(f"CPU ìµœì í™”: ë³€í™˜ ê°•ë„ë¥¼ {strength}ë¡œ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.")

        # ìŠ¤íƒ€ì¼ ë³´ì¡´ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì¡°ì •
        if preserve_style:
            # ì…ë ¥ ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ì„ ë¶„ì„í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
            style_enhanced_prompt = f"{prompt}, maintaining the artistic style and composition of the original image"
        else:
            style_enhanced_prompt = prompt

        print(f"ì´ë¯¸ì§€ ìƒì„± ì‹œì‘... (ë””ë°”ì´ìŠ¤: {device}, ìŠ¤í…: {num_inference_steps})")
        
        # ì´ë¯¸ì§€ ìƒì„±
        result = pipe(
            prompt=style_enhanced_prompt,
            negative_prompt=negative_prompt,
            image=input_image,
            strength=strength,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            generator=generator,
        )

        # ê²°ê³¼ ì´ë¯¸ì§€ ë°˜í™˜
        output_image = result.images[0]

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del pipe
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return output_image, f"ìƒì„± ì™„ë£Œ! ì‚¬ìš©ëœ ì‹œë“œ: {seed} (ë””ë°”ì´ìŠ¤: {device})"

    except Exception as e:
        return None, f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


def create_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""

    # í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œë“¤
    prompt_examples = [
        "a beautiful landscape with mountains and lake",
        "a futuristic city with flying cars",
        "a magical forest with glowing mushrooms",
        "a cozy coffee shop interior",
        "a space station orbiting Earth",
        "a medieval castle on a hill",
        "a tropical beach at sunset",
        "a cyberpunk street at night",
        "a peaceful Japanese garden",
        "a steampunk airship in the sky"
    ]

    # ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œë“¤
    negative_prompt_examples = [
        "blurry, low quality, distorted, ugly",
        "watermark, signature, text, logo",
        "oversaturated, overexposed, underexposed",
        "deformed, disfigured, bad anatomy",
        "noise, grain, artifacts"
    ]

    with gr.Blocks(
        title="Stable Diffusion 3.5 Style-Preserving Generation", 
        theme=gr.themes.Soft()
    ) as interface:
        gr.Markdown("# ğŸ¨ Stable Diffusion 3.5 ìŠ¤íƒ€ì¼ ë³´ì¡´ ì´ë¯¸ì§€ ìƒì„±")
        gr.Markdown("ì…ë ¥ ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•˜ë©´ì„œ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ë³´ì„¸ìš”!")

        with gr.Row():
            with gr.Column(scale=1):
                # ì…ë ¥ ì„¹ì…˜
                gr.Markdown("## ğŸ“¤ ì…ë ¥")
                input_image = gr.Image(
                    label="ì°¸ì¡°í•  ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", 
                    type="pil", 
                    height=300
                )

                prompt = gr.Textbox(
                    label="ìƒì„± í”„ë¡¬í”„íŠ¸",
                    placeholder="ì˜ˆ: a beautiful landscape with mountains and lake",
                    lines=3,
                )

                negative_prompt = gr.Textbox(
                    label="ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)",
                    placeholder="ì˜ˆ: blurry, low quality, distorted",
                    lines=2,
                )

                # íŒŒë¼ë¯¸í„° ì¡°ì •
                gr.Markdown("### âš™ï¸ íŒŒë¼ë¯¸í„° ì¡°ì •")
                with gr.Row():
                    strength = gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        value=0.6,
                        step=0.05,
                        label="ë³€í™˜ ê°•ë„",
                        info="ë†’ì„ìˆ˜ë¡ ë” ë§ì´ ë³€í™˜ë©ë‹ˆë‹¤",
                    )
                    guidance_scale = gr.Slider(
                        minimum=1.0,
                        maximum=20.0,
                        value=7.5,
                        step=0.5,
                        label="ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼",
                        info="ë†’ì„ìˆ˜ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ë” ì˜ ë”°ë¦…ë‹ˆë‹¤",
                    )

                with gr.Row():
                    num_inference_steps = gr.Slider(
                        minimum=10,
                        maximum=50,
                        value=25,
                        step=1,
                        label="ì¶”ë¡  ìŠ¤í… ìˆ˜",
                        info="ë†’ì„ìˆ˜ë¡ í’ˆì§ˆì´ ì¢‹ì•„ì§€ì§€ë§Œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤",
                    )
                    seed = gr.Number(
                        value=-1,
                        label="ëœë¤ ì‹œë“œ",
                        info="-1ì´ë©´ ëœë¤ ì‹œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤",
                    )

                # ìŠ¤íƒ€ì¼ ë³´ì¡´ ì˜µì…˜
                gr.Markdown("### ğŸ­ ìŠ¤íƒ€ì¼ ë³´ì¡´ ì„¤ì •")
                with gr.Row():
                    preserve_style = gr.Checkbox(
                        value=True,
                        label="ìŠ¤íƒ€ì¼ ë³´ì¡´ í™œì„±í™”",
                        info="ì…ë ¥ ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•©ë‹ˆë‹¤"
                    )
                    style_strength = gr.Slider(
                        minimum=0.0,
                        maximum=1.0,
                        value=0.8,
                        step=0.1,
                        label="ìŠ¤íƒ€ì¼ ë³´ì¡´ ê°•ë„",
                        info="ìŠ¤íƒ€ì¼ì„ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ë³´ì¡´í• ì§€ ì„¤ì •",
                        visible=True
                    )

                # CPU ìµœì í™” ì˜µì…˜
                gr.Markdown("### âš¡ CPU ìµœì í™” ì„¤ì •")
                with gr.Row():
                    cpu_optimization = gr.Checkbox(
                        value=not torch.cuda.is_available(),
                        label="CPU ìµœì í™” í™œì„±í™”",
                        info="CPU ì‚¬ìš©ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤"
                    )
                    max_image_size = gr.Slider(
                        minimum=512,
                        maximum=1024,
                        value=768 if torch.cuda.is_available() else 512,
                        step=64,
                        label="ìµœëŒ€ ì´ë¯¸ì§€ í¬ê¸°",
                        info="CPU ì‚¬ìš©ì‹œ ë” ì‘ì€ í¬ê¸°ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤"
                    )

                # ë””ë°”ì´ìŠ¤ ì •ë³´ í‘œì‹œ
                device_info = "GPU" if torch.cuda.is_available() else "CPU"
                gr.Markdown(f"**í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device_info}**")
                if not torch.cuda.is_available():
                    gr.Markdown("âš ï¸ **CPU ëª¨ë“œ**: ìƒì„± ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (5-15ë¶„)")

                # ìƒì„± ë²„íŠ¼
                generate_btn = gr.Button(
                    "ğŸ¨ ì´ë¯¸ì§€ ìƒì„±í•˜ê¸°", 
                    variant="primary", 
                    size="lg"
                )

                # ìƒíƒœ ë©”ì‹œì§€
                status_text = gr.Textbox(
                    label="ìƒíƒœ", 
                    interactive=False, 
                    lines=2
                )

            with gr.Column(scale=1):
                # ì¶œë ¥ ì„¹ì…˜
                gr.Markdown("## ğŸ“¤ ê²°ê³¼")
                output_image = gr.Image(
                    label="ìƒì„±ëœ ì´ë¯¸ì§€", 
                    height=400
                )

        # í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ
        gr.Markdown("## ğŸ’¡ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ")
        with gr.Row():
            for i, example in enumerate(prompt_examples[:5]):
                gr.Button(example, size="sm").click(
                    fn=lambda x=example: x, 
                    outputs=prompt
                )

        with gr.Row():
            for i, example in enumerate(prompt_examples[5:]):
                gr.Button(example, size="sm").click(
                    fn=lambda x=example: x, 
                    outputs=prompt
                )

        # ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ
        gr.Markdown("## ğŸš« ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ")
        with gr.Row():
            for i, example in enumerate(negative_prompt_examples):
                gr.Button(example, size="sm").click(
                    fn=lambda x=example: x, 
                    outputs=negative_prompt
                )

        # ì‚¬ìš©ë²• ì•ˆë‚´
        gr.Markdown(
            """
        ## ğŸ“– ì‚¬ìš©ë²•
        
        1. **ì´ë¯¸ì§€ ì—…ë¡œë“œ**: ìŠ¤íƒ€ì¼ì„ ì°¸ì¡°í•  ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”
        2. **í”„ë¡¬í”„íŠ¸ ì…ë ¥**: ìƒì„±í•˜ê³  ì‹¶ì€ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”
        3. **íŒŒë¼ë¯¸í„° ì¡°ì •**: ë³€í™˜ ê°•ë„ì™€ í’ˆì§ˆì„ ì¡°ì •í•˜ì„¸ìš”
        4. **ìŠ¤íƒ€ì¼ ë³´ì¡´ ì„¤ì •**: ì›ë³¸ ìŠ¤íƒ€ì¼ì„ ì–¼ë§ˆë‚˜ ìœ ì§€í• ì§€ ì„¤ì •í•˜ì„¸ìš”
        5. **CPU ìµœì í™”**: CPU ì‚¬ìš©ì‹œ ìµœì í™” ì˜µì…˜ì„ í™œì„±í™”í•˜ì„¸ìš”
        6. **ìƒì„± ì‹¤í–‰**: "ì´ë¯¸ì§€ ìƒì„±í•˜ê¸°" ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
        
        ### ğŸ’¡ íŒ
        - **ë³€í™˜ ê°•ë„**: 0.4-0.7 ì •ë„ê°€ ì ë‹¹í•©ë‹ˆë‹¤
        - **ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼**: 7-10 ì •ë„ê°€ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤
        - **ì¶”ë¡  ìŠ¤í…**: 20-30 ìŠ¤í…ì´ í’ˆì§ˆê³¼ ì†ë„ì˜ ê· í˜•ì ì…ë‹ˆë‹¤
        - **ìŠ¤íƒ€ì¼ ë³´ì¡´**: í™œì„±í™”í•˜ë©´ ì›ë³¸ ì´ë¯¸ì§€ì˜ ì•„íŠ¸ ìŠ¤íƒ€ì¼ì„ ìœ ì§€í•©ë‹ˆë‹¤
        
        ### âš¡ CPU ì‚¬ìš©ì‹œ ì£¼ì˜ì‚¬í•­
        - **ìƒì„± ì‹œê°„**: CPUì—ì„œëŠ” GPUë³´ë‹¤ í›¨ì”¬ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤ (5-15ë¶„)
        - **ë©”ëª¨ë¦¬**: ìµœì†Œ 8GB RAMì´ í•„ìš”í•©ë‹ˆë‹¤
        - **ì´ë¯¸ì§€ í¬ê¸°**: 512x512 ì´í•˜ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤
        - **ìµœì í™”**: CPU ìµœì í™” ì˜µì…˜ì„ í™œì„±í™”í•˜ë©´ ì•ˆì •ì„±ì´ í–¥ìƒë©ë‹ˆë‹¤
        """
        )

        # ì´ë²¤íŠ¸ ì—°ê²°
        generate_btn.click(
            fn=style_preserving_generation,
            inputs=[
                input_image,
                prompt,
                negative_prompt,
                strength,
                guidance_scale,
                num_inference_steps,
                seed,
                preserve_style,
                style_strength,
                cpu_optimization,
                max_image_size,
            ],
            outputs=[output_image, status_text],
        )

        # ìŠ¤íƒ€ì¼ ë³´ì¡´ ì²´í¬ë°•ìŠ¤ ì´ë²¤íŠ¸
        preserve_style.change(
            fn=lambda x: gr.update(visible=x),
            inputs=preserve_style,
            outputs=style_strength,
        )

    return interface


if __name__ == "__main__":
    # ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
    interface = create_interface()
    interface.launch()
