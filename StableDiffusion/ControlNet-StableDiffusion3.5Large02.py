import torch
import gradio as gr
from diffusers import StableDiffusion3ControlNetPipeline, SD3ControlNetModel
import cv2
import numpy as np
from PIL import Image

# ì „ì—­ ë³€ìˆ˜ë¡œ íŒŒì´í”„ë¼ì¸ ì €ì¥
pipe = None


# ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜
def initialize_models():
    """Stable Diffusion 3.5 Large ControlNet ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global pipe
    try:
        print("ControlNet ëª¨ë¸ì„ ë¡œë”©í•˜ëŠ” ì¤‘...")
        # SD3 ControlNet ëª¨ë¸ ë¡œë“œ
        controlnet = SD3ControlNetModel.from_pretrained(
            "stabilityai/stable-diffusion-3.5-large-controlnet-canny",
            torch_dtype=torch.float16,
        )

        print("Stable Diffusion 3.5 Large íŒŒì´í”„ë¼ì¸ì„ ë¡œë”©í•˜ëŠ” ì¤‘...")
        # SD 3.5 Large íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        pipe = StableDiffusion3ControlNetPipeline.from_pretrained(
            "stabilityai/stable-diffusion-3.5-large",
            controlnet=controlnet,
            torch_dtype=torch.float16,
        )

        # GPU ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° GPUë¡œ, ì•„ë‹ˆë©´ CPUë¡œ
        device = "cuda" if torch.cuda.is_available() else "cpu"
        pipe = pipe.to(device)

        print(f"ëª¨ë¸ì´ {device}ì— ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return True
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return False


# í•´ìƒë„ ì •ê·œí™” í•¨ìˆ˜
def normalize_resolution(width, height, base=16):
    """í•´ìƒë„ë¥¼ ì§€ì •ëœ ë°°ìˆ˜ë¡œ ì¡°ì •í•˜ë©´ì„œ ì›ë³¸ ë¹„ìœ¨ì„ ìµœëŒ€í•œ ìœ ì§€í•©ë‹ˆë‹¤."""
    # ì›ë³¸ ë¹„ìœ¨ ê³„ì‚°
    aspect_ratio = width / height

    # 16ì˜ ë°°ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼
    new_width = ((width + base // 2) // base) * base
    new_height = ((height + base // 2) // base) * base

    # ìµœì†Œ/ìµœëŒ€ í•´ìƒë„ ì œí•œ (ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ)
    min_res = 512
    max_res = 1536

    # í¬ê¸°ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš° ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ì¡°ì •
    if new_width > max_res or new_height > max_res:
        if new_width > new_height:
            # ê°€ë¡œê°€ ë” ê¸´ ê²½ìš°
            scale = max_res / new_width
            new_width = max_res
            new_height = int((new_height * scale + base // 2) // base) * base
        else:
            # ì„¸ë¡œê°€ ë” ê¸´ ê²½ìš°
            scale = max_res / new_height
            new_height = max_res
            new_width = int((new_width * scale + base // 2) // base) * base

    if new_width < min_res or new_height < min_res:
        if new_width < new_height:
            # ê°€ë¡œê°€ ë” ì§§ì€ ê²½ìš°
            scale = min_res / new_width
            new_width = min_res
            new_height = int((new_height * scale + base // 2) // base) * base
        else:
            # ì„¸ë¡œê°€ ë” ì§§ì€ ê²½ìš°
            scale = min_res / new_height
            new_height = min_res
            new_width = int((new_width * scale + base // 2) // base) * base

    # ìµœì¢… 16ì˜ ë°°ìˆ˜ ë³´ì¥
    new_width = (new_width // base) * base
    new_height = (new_height // base) * base

    # ìµœì†Œê°’ ì¬í™•ì¸
    new_width = max(min_res, new_width)
    new_height = max(min_res, new_height)

    return new_width, new_height


# Canny edge detection ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_canny(image, low_threshold=100, high_threshold=200):
    """ì…ë ¥ ì´ë¯¸ì§€ì— Canny edge detectionì„ ì ìš©í•©ë‹ˆë‹¤."""
    if image is None:
        return None

    try:
        # ì´ë¯¸ì§€ í¬ê¸° ì œì•½ ì¡°ê±´ í™•ì¸ ë° ë¹„ìœ¨ ìœ ì§€ ë¦¬ì‚¬ì´ì¦ˆ
        width, height = image.size
        original_aspect_ratio = width / height
        max_size = 1536  # ìµœëŒ€ í•´ìƒë„ ì œí•œ (2048ì—ì„œ ì¤„ì„)
        min_size = 512  # ìµœì†Œ í•´ìƒë„ ì œí•œ

        # í¬ê¸° ì¡°ì •ì´ í•„ìš”í•œ ê²½ìš° ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ì¡°ì •
        if width > max_size or height > max_size:
            # ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ìµœëŒ€ í¬ê¸°ì— ë§ì¶¤
            if width > height:
                new_width = max_size
                new_height = int(max_size / original_aspect_ratio)
            else:
                new_height = max_size
                new_width = int(max_size * original_aspect_ratio)

            # 16ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •
            new_width, new_height = normalize_resolution(new_width, new_height)
            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            print(
                f"ì´ë¯¸ì§€ í¬ê¸°ê°€ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {width}x{height} -> {new_width}x{new_height}"
            )
            print(
                f"ì›ë³¸ ë¹„ìœ¨: {original_aspect_ratio:.3f}, ì¡°ì • í›„ ë¹„ìœ¨: {new_width/new_height:.3f}"
            )
            width, height = new_width, new_height

        elif width < min_size and height < min_size:
            # ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ìµœì†Œ í¬ê¸°ì— ë§ì¶¤
            if width > height:
                new_height = min_size
                new_width = int(min_size * original_aspect_ratio)
            else:
                new_width = min_size
                new_height = int(min_size / original_aspect_ratio)

            # 16ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •
            new_width, new_height = normalize_resolution(new_width, new_height)
            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            print(
                f"ì´ë¯¸ì§€ í¬ê¸°ê°€ ë¹„ìœ¨ ìœ ì§€í•˜ë©° í™•ëŒ€ë˜ì—ˆìŠµë‹ˆë‹¤: {width}x{height} -> {new_width}x{new_height}"
            )
            print(
                f"ì›ë³¸ ë¹„ìœ¨: {original_aspect_ratio:.3f}, ì¡°ì • í›„ ë¹„ìœ¨: {new_width/new_height:.3f}"
            )
            width, height = new_width, new_height

        else:
            # 16ì˜ ë°°ìˆ˜ë¡œë§Œ ì¡°ì • (ë¹„ìœ¨ ìµœì†Œ ë³€í™”)
            normalized_width, normalized_height = normalize_resolution(width, height)
            if (normalized_width, normalized_height) != (width, height):
                image = image.resize(
                    (normalized_width, normalized_height), Image.Resampling.LANCZOS
                )
                print(
                    f"í•´ìƒë„ê°€ 16ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {width}x{height} -> {normalized_width}x{normalized_height}"
                )
                print(
                    f"ì›ë³¸ ë¹„ìœ¨: {original_aspect_ratio:.3f}, ì¡°ì • í›„ ë¹„ìœ¨: {normalized_width/normalized_height:.3f}"
                )
                width, height = normalized_width, normalized_height

        # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
        image_np = np.array(image)

        # ì´ë¯¸ì§€ ì±„ë„ í™•ì¸ ë° ì²˜ë¦¬
        if len(image_np.shape) == 4:  # RGBA
            # ì•ŒíŒŒ ì±„ë„ ì œê±°í•˜ê³  RGBë¡œ ë³€í™˜
            image_np = image_np[:, :, :3]
        elif len(image_np.shape) == 3 and image_np.shape[2] == 3:  # RGB
            pass  # ê·¸ëŒ€ë¡œ ì‚¬ìš©
        elif (
            len(image_np.shape) == 3 and image_np.shape[2] == 1
        ):  # Grayscale with 1 channel
            image_np = image_np.squeeze()
        elif len(image_np.shape) == 2:  # Pure Grayscale
            pass  # ê·¸ëŒ€ë¡œ ì‚¬ìš©
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹: {image_np.shape}")

        # RGBë¥¼ Grayscaleë¡œ ë³€í™˜
        if len(image_np.shape) == 3:
            gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
        else:
            gray = image_np

        # Canny edge detection ì ìš©
        canny = cv2.Canny(gray, low_threshold, high_threshold)

        # 3ì±„ë„ë¡œ ë³€í™˜ (RGB)
        canny_image = canny[:, :, None]
        canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)

        return Image.fromarray(canny_image)

    except Exception as e:
        print(f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


# ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜
def generate_image(
    input_image,
    prompt,
    negative_prompt="",
    num_inference_steps=25,
    guidance_scale=7.5,
    controlnet_conditioning_scale=1.0,
    low_threshold=100,
    high_threshold=200,
):
    """SD 3.5 Large ControlNetì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    global pipe

    try:
        if pipe is None:
            return None, "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”."

        if input_image is None:
            return None, "ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."

        # ì´ë¯¸ì§€ í˜•ì‹ ë° í¬ê¸° ê²€ì¦
        try:
            width, height = input_image.size
            if width * height > 1536 * 1536:  # ìµœëŒ€ í•´ìƒë„ ë³€ê²½
                return (
                    None,
                    "ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. 1536x1536 ì´í•˜ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.",
                )
            if width < 256 or height < 256:
                return (
                    None,
                    "ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ìµœì†Œ 256x256 í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.",
                )
        except Exception as e:
            return None, f"ì´ë¯¸ì§€ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {str(e)}"

        if not prompt.strip():
            return None, "í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

        # Canny edge detection ì ìš©
        control_image = preprocess_canny(
            input_image, int(low_threshold), int(high_threshold)
        )

        if control_image is None:
            return None, "Canny edge ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        # ì œì–´ ì´ë¯¸ì§€ì˜ í•´ìƒë„ë¥¼ ì¶œë ¥ í•´ìƒë„ë¡œ ì‚¬ìš©
        output_width, output_height = control_image.size
        print(f"ì¶œë ¥ í•´ìƒë„: {output_width}x{output_height}")

        # ì´ë¯¸ì§€ ìƒì„±
        with torch.no_grad():
            result = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt if negative_prompt.strip() else None,
                control_image=control_image,
                num_inference_steps=int(num_inference_steps),
                guidance_scale=float(guidance_scale),
                controlnet_conditioning_scale=float(controlnet_conditioning_scale),
                height=output_height,
                width=output_width,
            )

        generated_image = result.images[0]
        return generated_image, "ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!"

    except Exception as e:
        return None, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
def create_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""

    with gr.Blocks(
        title="SD 3.5 Large ControlNet Generator",
        theme=gr.themes.Soft(),
        css="""
        .main-container { max-width: 1200px; margin: 0 auto; }
        .status-box { background-color: #f0f8ff; padding: 10px; border-radius: 5px; }
        """,
    ) as demo:

        gr.Markdown(
            """
            # ğŸ¨ Stable Diffusion 3.5 Large + ControlNet ì´ë¯¸ì§€ ìƒì„±ê¸°
            """
        )

        with gr.Row():
            with gr.Column(scale=1):
                # ì…ë ¥ ì„¹ì…˜
                gr.Markdown("### ğŸ“¥ ì…ë ¥")
                input_image = gr.Image(
                    label="ì…ë ¥ ì´ë¯¸ì§€ (êµ¬ì¡° ì°¸ì¡°ìš©)", type="pil", height=350
                )

                prompt = gr.Textbox(
                    label="í”„ë¡¬í”„íŠ¸",
                    placeholder="ì˜ˆ: A beautiful landscape painting in impressionist style",
                    lines=3,
                    value="A photorealistic, high-quality, detailed, masterpiece, 8k resolution, professional photography, sharp focus, vivid colors, perfect composition, dramatic lighting, cinematic quality",
                )

                negative_prompt = gr.Textbox(
                    label="ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)",
                    placeholder="ì˜ˆ: blurry, low quality, distorted",
                    lines=2,
                    value="blurry, low quality, bad anatomy, distorted, ugly, deformed, poorly drawn, bad hands, bad fingers, missing limbs, extra limbs, cropped, worst quality, low resolution, jpeg artifacts, watermark, text, signature, username, over saturated, under saturated, overexposed, underexposed",
                )

                # ê³ ê¸‰ ì„¤ì •
                with gr.Accordion("ğŸ”§ ê³ ê¸‰ ì„¤ì •", open=False):
                    num_inference_steps = gr.Slider(
                        minimum=15,
                        maximum=50,
                        value=25,
                        step=1,
                        label="ì¶”ë¡  ë‹¨ê³„ ìˆ˜ (ë†’ì„ìˆ˜ë¡ í’ˆì§ˆ í–¥ìƒ, ì‹œê°„ ì¦ê°€)",
                    )

                    guidance_scale = gr.Slider(
                        minimum=1.0,
                        maximum=15.0,
                        value=7.5,
                        step=0.5,
                        label="ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ (í”„ë¡¬í”„íŠ¸ ì¤€ìˆ˜ë„)",
                    )

                    controlnet_conditioning_scale = gr.Slider(
                        minimum=0.0,
                        maximum=2.0,
                        value=1.0,
                        step=0.1,
                        label="ControlNet ê°•ë„ (êµ¬ì¡° ë”°ë¼í•˜ê¸° ì •ë„)",
                    )

                    with gr.Row():
                        low_threshold = gr.Slider(
                            minimum=50,
                            maximum=150,
                            value=100,
                            step=10,
                            label="Canny ë‚®ì€ ì„ê³„ê°’",
                        )

                        high_threshold = gr.Slider(
                            minimum=150,
                            maximum=300,
                            value=200,
                            step=10,
                            label="Canny ë†’ì€ ì„ê³„ê°’",
                        )

                generate_btn = gr.Button(
                    "ğŸ¨ ì´ë¯¸ì§€ ìƒì„±", variant="primary", size="lg", interactive=True
                )

            with gr.Column(scale=1):
                # ì¶œë ¥ ì„¹ì…˜
                gr.Markdown("### ğŸ“¤ ì¶œë ¥")

                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### Canny Edge ë¯¸ë¦¬ë³´ê¸°")
                        canny_output = gr.Image(label="Canny Edge", height=300)

                    with gr.Column():
                        gr.Markdown("#### ìƒì„±ëœ ì´ë¯¸ì§€")
                        output_image = gr.Image(label="ìƒì„± ê²°ê³¼", height=300)

                status_text = gr.Textbox(label="ìƒíƒœ", interactive=False, lines=2)

        # ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸ ë²„íŠ¼ë“¤
        with gr.Row():
            gr.Markdown("### ğŸ¯ ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸")

        with gr.Row():
            example_prompts = [
                "A majestic lion in a savanna, photorealistic, golden hour lighting",
                "A futuristic cyberpunk cityscape at night, neon lights, detailed",
                "A serene mountain landscape, oil painting style, peaceful atmosphere",
                "An elegant portrait of a woman, renaissance painting style, detailed",
            ]

            for i, prompt_text in enumerate(example_prompts):
                gr.Button(f"ì˜ˆì‹œ {i+1}", size="sm").click(
                    lambda p=prompt_text: p, outputs=[prompt]
                )

        # ì´ë²¤íŠ¸ í•¨ìˆ˜ë“¤
        def preview_canny(image, low_thresh, high_thresh):
            if image is None:
                return None
            return preprocess_canny(image, int(low_thresh), int(high_thresh))

        def generate_with_preview(*args):
            (
                input_img,
                prompt_text,
                neg_prompt,
                steps,
                guid_scale,
                ctrl_scale,
                low_thresh,
                high_thresh,
            ) = args

            # Canny ë¯¸ë¦¬ë³´ê¸° ìƒì„±
            canny_preview = None
            if input_img is not None:
                canny_preview = preprocess_canny(
                    input_img, int(low_thresh), int(high_thresh)
                )

            # ì´ë¯¸ì§€ ìƒì„± - ì˜¬ë°”ë¥¸ ì¸ìˆ˜ ìˆœì„œë¡œ í˜¸ì¶œ
            generated_img, status = generate_image(
                input_image=input_img,
                prompt=prompt_text,
                negative_prompt=neg_prompt,
                num_inference_steps=steps,
                guidance_scale=guid_scale,
                controlnet_conditioning_scale=ctrl_scale,
                low_threshold=low_thresh,
                high_threshold=high_thresh,
            )

            return canny_preview, generated_img, status

        # ì´ë²¤íŠ¸ ë°”ì¸ë”©
        # Canny ë¯¸ë¦¬ë³´ê¸° ì—…ë°ì´íŠ¸
        for component in [input_image, low_threshold, high_threshold]:
            component.change(
                fn=preview_canny,
                inputs=[input_image, low_threshold, high_threshold],
                outputs=[canny_output],
            )

        # ì´ë¯¸ì§€ ìƒì„±
        generate_btn.click(
            fn=generate_with_preview,
            inputs=[
                input_image,
                prompt,
                negative_prompt,
                num_inference_steps,
                guidance_scale,
                controlnet_conditioning_scale,
                low_threshold,
                high_threshold,
            ],
            outputs=[canny_output, output_image, status_text],
        )

    return demo


# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    print("ğŸš€ Stable Diffusion 3.5 Large ControlNet GUIë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ëª¨ë¸ ìë™ ì´ˆê¸°í™”
    print("ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
    success = initialize_models()
    if not success:
        print("âŒ ëª¨ë¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit(1)

    # Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
    demo = create_interface()
    demo.launch(share=False, inbrowser=True)
