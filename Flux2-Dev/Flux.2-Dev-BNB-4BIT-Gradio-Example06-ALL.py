import torch
from diffusers import Flux2Pipeline
from transformers import Mistral3ForConditionalGeneration, BitsAndBytesConfig
from datetime import datetime
from PIL import Image
import os
import warnings
import gradio as gr
import platform
import shutil
import signal
import sys
import gc
import time
import atexit


# Requirements :
# diffusers: 0.37.0.dev0 (latest git â€” has Flux2Pipeline)
# transformers: 4.53.2 (has HybridCache)
# bitsandbytes: 0.49.1 (upgraded from 0.46.1 â€” may fix the original metadata error)


try:
    import psutil
except Exception:
    psutil = None

# Set device and data type
if torch.cuda.is_available():
    device = "cuda"
    dtype = torch.bfloat16
elif torch.backends.mps.is_available():
    device = "mps"
    dtype = torch.bfloat16
else:
    device = "cpu"
    dtype = torch.float32


prompt_input = "The image is a high-quality, photorealistic cosplay portrait of a young Korean woman with a soft, idol aesthetic. Physical Appearance: Face: She has a fair, clear complexion. She is wearing striking bright blue contact lenses that contrast with her dark hair. Her expression is innocent and curious, looking directly at the camera: She has long, straight jet-black hair with thick, straight-cut bangs (fringe) that frame her face. Attire (Blue & White Bunny Theme): Headwear: She wears tall, upright blue fabric bunny ears with white lace inner lining and a delicate white lace headband base, accented with a small white bow. Outfit: She wears a unique blue denim-textured bodysuit. It features a front zipper, silver buttons, and thin silver chains draped across the chest. The sides are constructed from semi-sheer white lace. Accessories: Around her neck is a blue bow tie attached to a white collar. She wears long, white floral lace fingerless sleeves that extend past her elbows, finished with blue cuffs and small black decorative ribbons. Legwear: She wears white fishnet stockings held up by blue and white ruffled lace garters adorned with small white bows. Pose: She is standing gracefully in front of the edge of a light-colored, vintage-style bed or cushioned bench. Her body is slightly angled toward the camera, creating a soft and inviting posture. Setting & Background: Location: A bright, high-key studio set designed to look like a clean, airy bedroom. Background: The background is dominated by large windows with white vertical blinds or curtains, allowing soft, diffused natural-looking light to flood the scene. The background is softly blurred (bokeh). Lighting: The lighting is bright, soft, and even, minimizing harsh shadows and giving the skin a glowing, porcelain appearance. Flux Prompt Prompt: A photorealistic, high-quality cosplay portrait of a beautiful Korean woman dressed in a blue and white bunny girl outfit. She has long straight black hair with hime-cut bangs and vibrant blue eyes. She wears tall blue bunny ears with white lace trim, a blue denim-textured bodysuit with a front zipper and white lace side panels, a blue bow tie, and long white lace sleeves. She is standing in front of a white bed in a bright, sun-drenched room with soft-focus white curtains. She is looking at the camera with a soft, innocent expression.8k resolution, high-key lighting, cinematic soft focus, detailed textures of denim and lace, gravure photography style. Key Stylistic Keywords Blue bunny girl, denim cosplay, white lace, high-key lighting, blue contact lenses, black hair with bangs, fishnet stockings, airy atmosphere, photorealistic, innocent and alluring, studio photography."

print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device} | dtype: {dtype}")


def _bytes_to_gb(value_bytes):
    return f"{value_bytes / (1024 ** 3):.2f} GB"


def print_system_resources():
    print("=== ì‹œìŠ¤í…œ ìì› ì •ë³´ ===")
    print(f"OS: {platform.system()} {platform.release()} ({platform.machine()})")
    print(f"CPU ì½”ì–´: {os.cpu_count()}")

    if psutil is not None:
        mem = psutil.virtual_memory()
        print(
            f"RAM: {_bytes_to_gb(mem.available)} / {_bytes_to_gb(mem.total)} (ì‚¬ìš© ê°€ëŠ¥/ì „ì²´)"
        )
    else:
        try:
            page_size = os.sysconf("SC_PAGE_SIZE")
            phys_pages = os.sysconf("SC_PHYS_PAGES")
            total_ram = page_size * phys_pages
            print(f"RAM: {_bytes_to_gb(total_ram)} (ì „ì²´)")
        except Exception:
            print("RAM: ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    try:
        disk = shutil.disk_usage(os.getcwd())
        print(
            f"ë””ìŠ¤í¬: {_bytes_to_gb(disk.free)} / {_bytes_to_gb(disk.total)} (ì‚¬ìš© ê°€ëŠ¥/ì „ì²´)"
        )
    except Exception:
        print("ë””ìŠ¤í¬: ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if torch.cuda.is_available():
        try:
            props = torch.cuda.get_device_properties(0)
            total_vram = _bytes_to_gb(props.total_memory)
            allocated = _bytes_to_gb(torch.cuda.memory_allocated(0))
            reserved = _bytes_to_gb(torch.cuda.memory_reserved(0))
            print(
                f"CUDA GPU: {props.name} | VRAM: {allocated} (ì‚¬ìš©ì¤‘) / {reserved} (ì˜ˆì•½) / {total_vram} (ì „ì²´)"
            )
        except Exception:
            print("CUDA GPU: ì •ë³´ í™•ì¸ ì‹¤íŒ¨")
    elif torch.backends.mps.is_available():
        print("MPS: ì‚¬ìš© ê°€ëŠ¥ (GPU ë©”ëª¨ë¦¬ ì •ë³´ëŠ” ì§€ì›ë˜ì§€ ì•ŠìŒ)")


print_system_resources()


def cleanup_resources():
    global pipe, interface
    try:
        print("\n[ì¢…ë£Œ] ìì› í•´ì œ ì‹œì‘...")
        if "interface" in globals() and interface is not None:
            try:
                interface.close()
            except Exception:
                pass
        if "pipe" in globals() and pipe is not None:
            try:
                pipe.to("cpu")
            except Exception:
                pass
            pipe = None
        gc.collect()
        if torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass
        if hasattr(torch, "mps") and torch.backends.mps.is_available():
            try:
                torch.mps.empty_cache()
            except Exception:
                pass
        print("[ì¢…ë£Œ] ìì› í•´ì œ ì™„ë£Œ.")
    except Exception:
        print("[ì¢…ë£Œ] ìì› í•´ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ.")


def _handle_sigint(signum, frame):
    cleanup_resources()
    sys.exit(0)


signal.signal(signal.SIGINT, _handle_sigint)
atexit.register(cleanup_resources)

# Clear memory before loading models
print("[ì´ˆê¸°í™”] ëª¨ë¸ ë¡œë”© ì „ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
if hasattr(torch, "mps") and torch.backends.mps.is_available():
    torch.mps.empty_cache()
print("[ì´ˆê¸°í™”] ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ.")

# Actually, more RAM is required to run this program. Not working in 32GB. More than 48GB RAM required.
# Load text encoder separately with on-the-fly 4-bit quantization
# (the pre-quantized repo has incompatible bnb metadata for the T5 text encoder)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=dtype,
)
text_encoder = Mistral3ForConditionalGeneration.from_pretrained(
    "black-forest-labs/FLUX.2-dev",
    subfolder="text_encoder",
    quantization_config=bnb_config,
    torch_dtype=dtype,
)

# Load pipeline from pre-quantized repo, overriding the text encoder
repo_id = "diffusers/FLUX.2-dev-bnb-4bit"
pipe = Flux2Pipeline.from_pretrained(
    repo_id,
    text_encoder=text_encoder,
    torch_dtype=dtype,
    device_map="balanced",
)

if device == "cuda":
    print("Using CUDA device optimizations...")
    pipe.enable_model_cpu_offload()
    #pipe.enable_attention_slicing()
    #pipe.enable_sequential_cpu_offload()
elif device == "mps":
    print("Using MPS device optimizations...")
    print("No memory optimizations applied.")
else:
    print("Using CPU device optimizations...")
    pipe.enable_model_cpu_offload()
    pipe.enable_attention_slicing()
    pipe.enable_sequential_cpu_offload()


print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")


def generate_image(
    prompt,
    negative_prompt,
    width,
    height,
    guidance_scale,
    true_cfg_scale,
    num_inference_steps,
    seed,
    strength,
    progress=gr.Progress(track_tqdm=True),
):
    try:
        steps = int(num_inference_steps)
        start_time = time.time()

        # Print generation parameters to CLI
        print("=" * 60)
        print("=== ì´ë¯¸ì§€ ìƒì„± íŒŒë¼ë¯¸í„° ===")
        print(f"  í”„ë¡¬í”„íŠ¸: {prompt[:100]}{'...' if len(prompt) > 100 else ''}")
        if negative_prompt and negative_prompt.strip() and true_cfg_scale > 1.0:
            print(
                f"  ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸: {negative_prompt.strip()[:100]}{'...' if len(negative_prompt.strip()) > 100 else ''}"
            )
            print(f"  True CFG Scale: {true_cfg_scale}")
        else:
            print("  ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸: (ë¹„í™œì„± - True CFG Scaleì´ 1.0)")
        print(f"  ì´ë¯¸ì§€ í¬ê¸°: {width}x{height}")
        print(f"  Guidance Scale: {guidance_scale}")
        print(f"  ì¶”ë¡  ìŠ¤í…: {steps}")
        print(f"  ì‹œë“œ: {int(seed)}")
        print(f"  ê°•ë„: {strength}")
        print(f"  ë””ë°”ì´ìŠ¤: {device} | dtype: {dtype}")
        print("=" * 60)

        progress(0.0, desc="ì´ë¯¸ì§€ ìƒì„± ì¤€ë¹„ ì¤‘...")
        print("ì´ë¯¸ì§€ ìƒì„± ì‹œì‘...")

        # Callback to report each inference step to Gradio progress bar and CLI status bar
        def step_callback(_pipe, step_index, _timestep, callback_kwargs):
            current = step_index + 1
            elapsed = time.time() - start_time
            ratio = current / steps
            progress_val = 0.05 + ratio * 0.85
            progress(
                progress_val, desc=f"ì¶”ë¡  ìŠ¤í… {current}/{steps} ({elapsed:.1f}ì´ˆ ê²½ê³¼)"
            )

            # CLI status bar
            bar_len = 30
            filled = int(bar_len * ratio)
            bar = "\u2588" * filled + "\u2591" * (bar_len - filled)
            speed = elapsed / current
            eta = speed * (steps - current)
            print(
                f"\r  [{bar}] {current}/{steps} ({ratio*100:.0f}%) | "
                f"{elapsed:.1f}s elapsed | ETA {eta:.1f}s | {speed:.2f}s/step",
                end="",
                flush=True,
            )
            if current == steps:
                print()
            return callback_kwargs

        # Build pipeline arguments
        pipe_kwargs = dict(
            prompt=prompt,
            width=width,
            height=height,
            guidance_scale=guidance_scale,
            num_inference_steps=steps,
            generator=torch.Generator(device=device).manual_seed(seed),
            callback_on_step_end=step_callback,
        )

        # Add negative prompt when provided and true_cfg_scale > 1
        if negative_prompt and negative_prompt.strip() and true_cfg_scale > 1.0:
            pipe_kwargs["negative_prompt"] = negative_prompt.strip()
            pipe_kwargs["true_cfg_scale"] = true_cfg_scale

        progress(0.05, desc="ì¶”ë¡  ì‹œì‘...")

        # Run the pipeline
        image = pipe(**pipe_kwargs).images[0]

        elapsed = time.time() - start_time
        progress(0.95, desc="ì´ë¯¸ì§€ ì €ì¥ ì¤‘...")

        # Save with timestamp and parameters
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        script_name = os.path.splitext(os.path.basename(__file__))[0]
        filename = f"{script_name}_{timestamp}_{width}x{height}_gs{guidance_scale}_tcfg{true_cfg_scale}_step{steps}_seed{int(seed)}_str{strength}.png"
        image.save(filename)

        print(f"âœ“ ì €ì¥ ì™„ë£Œ: {filename} ({elapsed:.1f}ì´ˆ)")
        progress(1.0, desc="ì™„ë£Œ!")
        return image, f"âœ“ ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename} ({elapsed:.1f}ì´ˆ)"
    except Exception as e:
        return None, f"âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# Create Gradio interface
with gr.Blocks(title="Flux.2-dev Image Generator") as interface:
    gr.Markdown("# ğŸ¨ Flux.2-dev Image Generator")
    gr.Markdown("AIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    with gr.Row():
        with gr.Column(scale=1):
            # Input parameters
            prompt = gr.Textbox(
                label="í”„ë¡¬í”„íŠ¸",
                value=prompt_input,
                lines=3,
                placeholder="ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª…ì„ ì…ë ¥í•˜ì„¸ìš”.",
                info="ìƒì„±í•˜ê³  ì‹¶ì€ ì´ë¯¸ì§€ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ì„¤ëª…ì…ë‹ˆë‹¤. ìì„¸í• ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤. ì˜ˆ: 'ì—¬ì, ë¯¸ì†Œ, í•´ë³€, ë¹¨ê°„ ë¹„í‚¤ë‹ˆ'",
            )
            negative_prompt = gr.Textbox(
                label="ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸",
                value="blurry, low quality, deformed, ugly, bad anatomy, disfigured, poorly drawn face, mutation, extra limbs, extra fingers, missing fingers, watermark, text, signature",
                lines=2,
                placeholder="ì›í•˜ì§€ ì•ŠëŠ” ìš”ì†Œë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì˜ˆ: 'blurry, low quality, deformed'",
                info="ì´ë¯¸ì§€ì—ì„œ ì œì™¸í•˜ê³  ì‹¶ì€ ìš”ì†Œë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤. True CFG Scaleì´ 1.0ë³´ë‹¤ í´ ë•Œë§Œ ì ìš©ë©ë‹ˆë‹¤.",
            )

            with gr.Row():
                width = gr.Slider(
                    label="ì´ë¯¸ì§€ ë„ˆë¹„",
                    minimum=256,
                    maximum=2048,
                    step=64,
                    value=768,
                    info="ìƒì„±í•  ì´ë¯¸ì§€ì˜ ë„ˆë¹„ë¥¼ ì§€ì •í•©ë‹ˆë‹¤ (í”½ì…€). 64ì˜ ë°°ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.",
                )
                height = gr.Slider(
                    label="ì´ë¯¸ì§€ ë†’ì´",
                    minimum=256,
                    maximum=2048,
                    step=64,
                    value=2048,
                    info="ìƒì„±í•  ì´ë¯¸ì§€ì˜ ë†’ì´ë¥¼ ì§€ì •í•©ë‹ˆë‹¤ (í”½ì…€). 64ì˜ ë°°ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.",
                )

            with gr.Row():
                guidance_scale = gr.Slider(
                    label="Guidance Scale (í”„ë¡¬í”„íŠ¸ ê°•ë„)",
                    minimum=1.0,
                    maximum=20.0,
                    step=0.5,
                    value=4.0,
                    info="ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ì–¼ë§ˆë‚˜ ë”°ë¥¼ì§€ ì œì–´í•©ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì°½ì˜ì , ë†’ì„ìˆ˜ë¡ ì •í™•í•©ë‹ˆë‹¤. ê¶Œì¥: 4-15",
                )
                true_cfg_scale = gr.Slider(
                    label="True CFG Scale (ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ê°•ë„)",
                    minimum=1.0,
                    maximum=10.0,
                    step=0.5,
                    value=1.0,
                    info="ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ì˜ ê°•ë„ì…ë‹ˆë‹¤. 1.0ì´ë©´ ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. ê¶Œì¥: 1.5-3.0",
                )

            with gr.Row():
                num_inference_steps = gr.Slider(
                    label="ì¶”ë¡  ìŠ¤í…",
                    minimum=10,
                    maximum=50,
                    step=1,
                    value=28,
                    info="ì´ë¯¸ì§€ ìƒì„± ê³¼ì •ì˜ ë‹¨ê³„ ìˆ˜ì…ë‹ˆë‹¤. ë†’ì„ìˆ˜ë¡ í’ˆì§ˆì´ ì¢‹ì§€ë§Œ ì‹œê°„ì´ ë” ê±¸ë¦½ë‹ˆë‹¤. ê¶Œì¥: 20-28",
                )

            with gr.Row():
                seed = gr.Number(
                    label="ì‹œë“œ",
                    value=42,
                    precision=0,
                    info="ë‚œìˆ˜ ìƒì„±ì˜ ì‹œì‘ì ì…ë‹ˆë‹¤. ê°™ì€ ì‹œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.",
                )
                strength = gr.Slider(
                    label="ê°•ë„",
                    minimum=0.1,
                    maximum=1.0,
                    step=0.01,
                    value=0.85,
                    info="ìƒì„± ëª¨ë¸ì˜ ê°•ë„ë¥¼ ì œì–´í•©ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ê²°ê³¼, ë†’ì„ìˆ˜ë¡ ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼ì…ë‹ˆë‹¤.",
                )

            generate_btn = gr.Button("ğŸš€ ì´ë¯¸ì§€ ìƒì„±", variant="primary", size="lg")

        with gr.Column(scale=1):
            # Output
            output_image = gr.Image(label="ìƒì„±ëœ ì´ë¯¸ì§€", height=800)
            output_message = gr.Textbox(label="ìƒíƒœ", interactive=False)

    # Connect the generate button to the function
    generate_btn.click(
        fn=generate_image,
        inputs=[
            prompt,
            negative_prompt,
            width,
            height,
            guidance_scale,
            true_cfg_scale,
            num_inference_steps,
            seed,
            strength,
        ],
        outputs=[output_image, output_message],
    )

# Launch the interface
if __name__ == "__main__":
    interface.launch(inbrowser=True)
