{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J1twgYhHY2rO"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jfyTFXDaY5ZE"
   },
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, rows, cols, channels, z = 10):\n",
    "        # Input shape\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=256, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/dcgan_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrWXbWE_ZCuv",
    "outputId": "6345260f-64be-4f96-b00b-acb493814c7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              68992     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 291,713\n",
      "Trainable params: 291,329\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN(28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.350762, acc.: 27.15%] [G loss: 0.713632]\n",
      "1 [D loss: 0.762569, acc.: 57.81%] [G loss: 0.765508]\n",
      "2 [D loss: 0.457692, acc.: 79.30%] [G loss: 0.784620]\n",
      "3 [D loss: 0.202889, acc.: 95.70%] [G loss: 0.671304]\n",
      "4 [D loss: 0.114810, acc.: 99.02%] [G loss: 0.434000]\n",
      "5 [D loss: 0.099880, acc.: 99.02%] [G loss: 0.241731]\n",
      "6 [D loss: 1.067198, acc.: 52.54%] [G loss: 0.310778]\n",
      "7 [D loss: 0.929027, acc.: 53.71%] [G loss: 0.800647]\n",
      "8 [D loss: 0.787506, acc.: 54.88%] [G loss: 0.544773]\n",
      "9 [D loss: 0.883833, acc.: 48.24%] [G loss: 0.240922]\n",
      "10 [D loss: 0.917518, acc.: 49.80%] [G loss: 0.296264]\n",
      "11 [D loss: 0.961789, acc.: 41.80%] [G loss: 0.482976]\n",
      "12 [D loss: 0.947386, acc.: 41.80%] [G loss: 0.640525]\n",
      "13 [D loss: 0.988115, acc.: 42.38%] [G loss: 0.837099]\n",
      "14 [D loss: 0.910979, acc.: 44.14%] [G loss: 1.092490]\n",
      "15 [D loss: 0.896061, acc.: 46.09%] [G loss: 1.259088]\n",
      "16 [D loss: 0.826637, acc.: 50.00%] [G loss: 1.197489]\n",
      "17 [D loss: 1.014793, acc.: 42.38%] [G loss: 1.293638]\n",
      "18 [D loss: 0.928239, acc.: 41.99%] [G loss: 1.451895]\n",
      "19 [D loss: 0.892313, acc.: 46.88%] [G loss: 1.266949]\n",
      "20 [D loss: 0.861547, acc.: 49.22%] [G loss: 1.265401]\n",
      "21 [D loss: 0.752990, acc.: 56.25%] [G loss: 1.461650]\n",
      "22 [D loss: 0.796526, acc.: 54.49%] [G loss: 1.667874]\n",
      "23 [D loss: 0.752435, acc.: 58.40%] [G loss: 1.581296]\n",
      "24 [D loss: 0.768162, acc.: 56.45%] [G loss: 1.416234]\n",
      "25 [D loss: 0.825772, acc.: 52.54%] [G loss: 1.436504]\n",
      "26 [D loss: 0.921695, acc.: 47.66%] [G loss: 1.330571]\n",
      "27 [D loss: 0.937429, acc.: 42.97%] [G loss: 1.560086]\n",
      "28 [D loss: 1.028507, acc.: 41.21%] [G loss: 1.573522]\n",
      "29 [D loss: 0.950127, acc.: 40.43%] [G loss: 1.585112]\n",
      "30 [D loss: 0.961005, acc.: 40.43%] [G loss: 1.367756]\n",
      "31 [D loss: 1.069609, acc.: 38.09%] [G loss: 1.339222]\n",
      "32 [D loss: 0.956662, acc.: 40.62%] [G loss: 1.257514]\n",
      "33 [D loss: 0.978329, acc.: 43.16%] [G loss: 1.324169]\n",
      "34 [D loss: 0.866321, acc.: 48.83%] [G loss: 1.389398]\n",
      "35 [D loss: 0.897150, acc.: 49.80%] [G loss: 1.255312]\n",
      "36 [D loss: 0.844946, acc.: 50.98%] [G loss: 1.395380]\n",
      "37 [D loss: 0.876244, acc.: 52.15%] [G loss: 1.331741]\n",
      "38 [D loss: 0.886947, acc.: 47.27%] [G loss: 1.222448]\n",
      "39 [D loss: 0.907481, acc.: 49.02%] [G loss: 1.345024]\n",
      "40 [D loss: 0.903309, acc.: 46.09%] [G loss: 1.309795]\n",
      "41 [D loss: 0.892432, acc.: 45.51%] [G loss: 1.222356]\n",
      "42 [D loss: 0.874203, acc.: 46.09%] [G loss: 1.153104]\n",
      "43 [D loss: 0.874880, acc.: 46.68%] [G loss: 1.170497]\n",
      "44 [D loss: 0.771561, acc.: 52.54%] [G loss: 1.303503]\n",
      "45 [D loss: 0.706028, acc.: 60.16%] [G loss: 1.373122]\n",
      "46 [D loss: 0.674776, acc.: 59.77%] [G loss: 1.388595]\n",
      "47 [D loss: 0.621055, acc.: 67.77%] [G loss: 1.442016]\n",
      "48 [D loss: 0.691454, acc.: 61.91%] [G loss: 1.347100]\n",
      "49 [D loss: 0.757031, acc.: 58.01%] [G loss: 1.253485]\n",
      "50 [D loss: 0.757013, acc.: 56.05%] [G loss: 1.218739]\n",
      "51 [D loss: 0.925305, acc.: 43.55%] [G loss: 1.202020]\n",
      "52 [D loss: 1.106324, acc.: 32.23%] [G loss: 1.224759]\n",
      "53 [D loss: 1.002320, acc.: 39.26%] [G loss: 1.188120]\n",
      "54 [D loss: 1.020840, acc.: 37.11%] [G loss: 1.163150]\n",
      "55 [D loss: 1.070677, acc.: 33.40%] [G loss: 1.166531]\n",
      "56 [D loss: 0.939295, acc.: 41.21%] [G loss: 1.294188]\n",
      "57 [D loss: 0.940479, acc.: 41.02%] [G loss: 1.262853]\n",
      "58 [D loss: 0.893537, acc.: 45.31%] [G loss: 1.509273]\n",
      "59 [D loss: 0.889145, acc.: 44.73%] [G loss: 1.366337]\n",
      "60 [D loss: 0.910145, acc.: 41.41%] [G loss: 1.524696]\n",
      "61 [D loss: 1.012826, acc.: 39.26%] [G loss: 1.398312]\n",
      "62 [D loss: 1.008216, acc.: 38.67%] [G loss: 1.370222]\n",
      "63 [D loss: 0.936740, acc.: 42.97%] [G loss: 1.231956]\n",
      "64 [D loss: 0.939770, acc.: 41.99%] [G loss: 1.186007]\n",
      "65 [D loss: 0.886956, acc.: 44.92%] [G loss: 1.131278]\n",
      "66 [D loss: 0.951437, acc.: 39.45%] [G loss: 1.091911]\n",
      "67 [D loss: 0.905259, acc.: 44.14%] [G loss: 1.004115]\n",
      "68 [D loss: 0.910195, acc.: 42.97%] [G loss: 1.029288]\n",
      "69 [D loss: 0.814369, acc.: 50.98%] [G loss: 1.052573]\n",
      "70 [D loss: 0.751255, acc.: 56.05%] [G loss: 1.097555]\n",
      "71 [D loss: 0.720105, acc.: 56.05%] [G loss: 1.164987]\n",
      "72 [D loss: 0.736649, acc.: 56.64%] [G loss: 1.117756]\n",
      "73 [D loss: 0.747529, acc.: 55.86%] [G loss: 1.074377]\n",
      "74 [D loss: 0.750149, acc.: 52.93%] [G loss: 1.121493]\n",
      "75 [D loss: 0.815960, acc.: 48.83%] [G loss: 1.140401]\n",
      "76 [D loss: 0.933437, acc.: 41.21%] [G loss: 1.163478]\n",
      "77 [D loss: 0.932920, acc.: 44.14%] [G loss: 1.145738]\n",
      "78 [D loss: 0.874303, acc.: 44.73%] [G loss: 1.240345]\n",
      "79 [D loss: 0.952759, acc.: 39.26%] [G loss: 1.041547]\n",
      "80 [D loss: 0.888554, acc.: 43.55%] [G loss: 1.191147]\n",
      "81 [D loss: 0.962204, acc.: 36.91%] [G loss: 1.115114]\n",
      "82 [D loss: 0.820895, acc.: 48.44%] [G loss: 1.130600]\n",
      "83 [D loss: 0.798170, acc.: 50.00%] [G loss: 1.169373]\n",
      "84 [D loss: 0.811131, acc.: 49.80%] [G loss: 1.179073]\n",
      "85 [D loss: 0.769930, acc.: 53.91%] [G loss: 1.027664]\n",
      "86 [D loss: 0.803489, acc.: 49.80%] [G loss: 1.017466]\n",
      "87 [D loss: 0.709607, acc.: 59.18%] [G loss: 1.075929]\n",
      "88 [D loss: 0.716363, acc.: 57.42%] [G loss: 1.138569]\n",
      "89 [D loss: 0.728218, acc.: 55.66%] [G loss: 1.003166]\n",
      "90 [D loss: 0.739490, acc.: 56.45%] [G loss: 1.013940]\n",
      "91 [D loss: 0.764704, acc.: 53.71%] [G loss: 1.097965]\n",
      "92 [D loss: 0.831610, acc.: 48.63%] [G loss: 1.125849]\n",
      "93 [D loss: 0.842058, acc.: 47.85%] [G loss: 1.151244]\n",
      "94 [D loss: 0.852130, acc.: 46.68%] [G loss: 1.171301]\n",
      "95 [D loss: 0.888695, acc.: 43.55%] [G loss: 1.179178]\n",
      "96 [D loss: 0.900648, acc.: 42.77%] [G loss: 1.121601]\n",
      "97 [D loss: 0.798611, acc.: 49.80%] [G loss: 1.067299]\n",
      "98 [D loss: 0.902176, acc.: 39.45%] [G loss: 1.063196]\n",
      "99 [D loss: 0.922355, acc.: 40.23%] [G loss: 1.096859]\n",
      "100 [D loss: 0.817064, acc.: 46.48%] [G loss: 1.140538]\n",
      "101 [D loss: 0.833755, acc.: 48.63%] [G loss: 1.121617]\n",
      "102 [D loss: 0.819193, acc.: 49.41%] [G loss: 1.118356]\n",
      "103 [D loss: 0.786031, acc.: 50.78%] [G loss: 1.149488]\n",
      "104 [D loss: 0.788724, acc.: 51.76%] [G loss: 1.178257]\n",
      "105 [D loss: 0.735350, acc.: 55.86%] [G loss: 1.075276]\n",
      "106 [D loss: 0.815699, acc.: 48.83%] [G loss: 1.062962]\n",
      "107 [D loss: 0.851836, acc.: 44.53%] [G loss: 0.994748]\n",
      "108 [D loss: 0.856957, acc.: 44.53%] [G loss: 1.102741]\n",
      "109 [D loss: 0.879934, acc.: 42.38%] [G loss: 1.126461]\n",
      "110 [D loss: 0.982670, acc.: 36.72%] [G loss: 1.082503]\n",
      "111 [D loss: 0.895348, acc.: 39.65%] [G loss: 1.116791]\n",
      "112 [D loss: 0.883697, acc.: 44.92%] [G loss: 1.001210]\n",
      "113 [D loss: 0.825083, acc.: 47.07%] [G loss: 1.122169]\n",
      "114 [D loss: 0.943679, acc.: 39.26%] [G loss: 1.102646]\n",
      "115 [D loss: 0.878045, acc.: 42.38%] [G loss: 1.093670]\n",
      "116 [D loss: 0.915015, acc.: 39.65%] [G loss: 1.066241]\n",
      "117 [D loss: 0.877398, acc.: 44.53%] [G loss: 1.081362]\n",
      "118 [D loss: 0.790001, acc.: 49.61%] [G loss: 1.037163]\n",
      "119 [D loss: 0.784913, acc.: 50.00%] [G loss: 0.928258]\n",
      "120 [D loss: 0.723369, acc.: 55.47%] [G loss: 0.953934]\n",
      "121 [D loss: 0.724963, acc.: 59.96%] [G loss: 0.971014]\n",
      "122 [D loss: 0.742921, acc.: 56.84%] [G loss: 1.008985]\n",
      "123 [D loss: 0.722220, acc.: 55.86%] [G loss: 0.962454]\n",
      "124 [D loss: 0.844639, acc.: 45.31%] [G loss: 1.015805]\n",
      "125 [D loss: 0.802267, acc.: 53.91%] [G loss: 1.062380]\n",
      "126 [D loss: 0.904716, acc.: 44.34%] [G loss: 1.040556]\n",
      "127 [D loss: 0.893211, acc.: 40.43%] [G loss: 1.072148]\n",
      "128 [D loss: 0.964840, acc.: 36.13%] [G loss: 1.104269]\n",
      "129 [D loss: 0.867510, acc.: 43.95%] [G loss: 1.143019]\n",
      "130 [D loss: 0.889166, acc.: 44.53%] [G loss: 1.121043]\n",
      "131 [D loss: 0.745764, acc.: 53.12%] [G loss: 1.044143]\n",
      "132 [D loss: 0.698626, acc.: 60.55%] [G loss: 0.937660]\n",
      "133 [D loss: 0.633214, acc.: 64.45%] [G loss: 0.961429]\n",
      "134 [D loss: 0.650998, acc.: 63.87%] [G loss: 0.989640]\n",
      "135 [D loss: 0.714488, acc.: 55.66%] [G loss: 0.995773]\n",
      "136 [D loss: 0.718713, acc.: 58.01%] [G loss: 1.032908]\n",
      "137 [D loss: 0.878843, acc.: 45.70%] [G loss: 1.070183]\n",
      "138 [D loss: 0.918462, acc.: 41.21%] [G loss: 1.110822]\n",
      "139 [D loss: 0.964150, acc.: 35.94%] [G loss: 1.145389]\n",
      "140 [D loss: 0.960894, acc.: 38.28%] [G loss: 1.256171]\n",
      "141 [D loss: 0.979146, acc.: 35.16%] [G loss: 1.098071]\n",
      "142 [D loss: 0.933770, acc.: 38.67%] [G loss: 1.065087]\n",
      "143 [D loss: 0.863525, acc.: 43.95%] [G loss: 1.132590]\n",
      "144 [D loss: 0.891677, acc.: 42.58%] [G loss: 1.154898]\n",
      "145 [D loss: 0.863885, acc.: 44.14%] [G loss: 1.182346]\n",
      "146 [D loss: 0.760380, acc.: 54.10%] [G loss: 1.073937]\n",
      "147 [D loss: 0.779949, acc.: 54.69%] [G loss: 1.083524]\n",
      "148 [D loss: 0.825387, acc.: 48.83%] [G loss: 0.821541]\n",
      "149 [D loss: 0.769854, acc.: 53.71%] [G loss: 0.816111]\n",
      "150 [D loss: 0.798991, acc.: 52.73%] [G loss: 0.699530]\n",
      "151 [D loss: 0.789284, acc.: 50.59%] [G loss: 0.709310]\n",
      "152 [D loss: 0.694387, acc.: 60.55%] [G loss: 0.672007]\n",
      "153 [D loss: 0.699136, acc.: 56.84%] [G loss: 0.774884]\n",
      "154 [D loss: 0.570604, acc.: 70.70%] [G loss: 0.878110]\n",
      "155 [D loss: 0.578986, acc.: 67.38%] [G loss: 0.962590]\n",
      "156 [D loss: 0.655564, acc.: 61.52%] [G loss: 1.103124]\n",
      "157 [D loss: 0.768209, acc.: 53.91%] [G loss: 1.179361]\n",
      "158 [D loss: 0.836801, acc.: 46.68%] [G loss: 1.327181]\n",
      "159 [D loss: 0.787253, acc.: 50.98%] [G loss: 1.418196]\n",
      "160 [D loss: 0.889971, acc.: 42.77%] [G loss: 1.132503]\n",
      "161 [D loss: 0.806844, acc.: 50.39%] [G loss: 1.211983]\n",
      "162 [D loss: 0.885025, acc.: 41.41%] [G loss: 1.078590]\n",
      "163 [D loss: 0.803839, acc.: 46.68%] [G loss: 1.072930]\n",
      "164 [D loss: 0.807054, acc.: 53.12%] [G loss: 0.966524]\n",
      "165 [D loss: 0.868013, acc.: 45.31%] [G loss: 0.916497]\n",
      "166 [D loss: 0.827730, acc.: 44.53%] [G loss: 0.966742]\n",
      "167 [D loss: 0.729052, acc.: 55.47%] [G loss: 1.099457]\n",
      "168 [D loss: 0.705073, acc.: 56.64%] [G loss: 1.159321]\n",
      "169 [D loss: 0.860487, acc.: 46.48%] [G loss: 1.177715]\n",
      "170 [D loss: 0.783876, acc.: 50.39%] [G loss: 1.143347]\n",
      "171 [D loss: 0.827212, acc.: 44.92%] [G loss: 1.088440]\n",
      "172 [D loss: 0.717649, acc.: 55.27%] [G loss: 1.112835]\n",
      "173 [D loss: 0.822357, acc.: 45.70%] [G loss: 1.090042]\n",
      "174 [D loss: 0.719456, acc.: 53.91%] [G loss: 1.041427]\n",
      "175 [D loss: 0.773940, acc.: 50.78%] [G loss: 0.988927]\n",
      "176 [D loss: 0.784851, acc.: 48.44%] [G loss: 0.924106]\n",
      "177 [D loss: 0.815597, acc.: 45.51%] [G loss: 0.928835]\n",
      "178 [D loss: 0.871209, acc.: 43.55%] [G loss: 0.973627]\n",
      "179 [D loss: 0.896968, acc.: 43.36%] [G loss: 1.087953]\n",
      "180 [D loss: 0.919836, acc.: 37.50%] [G loss: 1.034245]\n",
      "181 [D loss: 0.924243, acc.: 38.28%] [G loss: 0.954229]\n",
      "182 [D loss: 0.937425, acc.: 41.21%] [G loss: 1.066681]\n",
      "183 [D loss: 0.774647, acc.: 52.15%] [G loss: 1.056481]\n",
      "184 [D loss: 0.815301, acc.: 49.22%] [G loss: 0.890850]\n",
      "185 [D loss: 0.641904, acc.: 63.48%] [G loss: 0.808592]\n",
      "186 [D loss: 0.608144, acc.: 67.77%] [G loss: 0.798273]\n",
      "187 [D loss: 0.453144, acc.: 79.30%] [G loss: 0.820618]\n",
      "188 [D loss: 0.462128, acc.: 79.49%] [G loss: 0.882994]\n",
      "189 [D loss: 0.444588, acc.: 81.25%] [G loss: 0.969683]\n",
      "190 [D loss: 0.451855, acc.: 83.40%] [G loss: 1.023731]\n",
      "191 [D loss: 0.643231, acc.: 62.70%] [G loss: 1.115551]\n",
      "192 [D loss: 0.825595, acc.: 47.46%] [G loss: 1.295339]\n",
      "193 [D loss: 0.924309, acc.: 39.84%] [G loss: 1.445012]\n",
      "194 [D loss: 0.907337, acc.: 41.99%] [G loss: 1.537826]\n",
      "195 [D loss: 0.972901, acc.: 35.94%] [G loss: 1.301386]\n",
      "196 [D loss: 0.812199, acc.: 47.46%] [G loss: 1.236585]\n",
      "197 [D loss: 0.756935, acc.: 55.47%] [G loss: 1.101172]\n",
      "198 [D loss: 0.691981, acc.: 58.79%] [G loss: 0.953697]\n",
      "199 [D loss: 0.586719, acc.: 67.77%] [G loss: 0.914845]\n",
      "200 [D loss: 0.519501, acc.: 75.78%] [G loss: 0.913162]\n",
      "201 [D loss: 0.514185, acc.: 75.59%] [G loss: 0.982530]\n",
      "202 [D loss: 0.473295, acc.: 79.49%] [G loss: 0.959759]\n",
      "203 [D loss: 0.639912, acc.: 63.28%] [G loss: 0.974813]\n",
      "204 [D loss: 0.637950, acc.: 66.21%] [G loss: 1.085403]\n",
      "205 [D loss: 0.744428, acc.: 54.49%] [G loss: 1.176320]\n",
      "206 [D loss: 0.933809, acc.: 37.70%] [G loss: 1.171245]\n",
      "207 [D loss: 0.952242, acc.: 36.72%] [G loss: 1.247774]\n",
      "208 [D loss: 0.910657, acc.: 39.26%] [G loss: 1.218162]\n",
      "209 [D loss: 1.063137, acc.: 29.10%] [G loss: 1.141573]\n",
      "210 [D loss: 0.980031, acc.: 35.55%] [G loss: 1.163116]\n",
      "211 [D loss: 0.859549, acc.: 45.51%] [G loss: 1.023211]\n",
      "212 [D loss: 0.788161, acc.: 49.61%] [G loss: 0.913505]\n",
      "213 [D loss: 0.731393, acc.: 52.54%] [G loss: 0.827224]\n",
      "214 [D loss: 0.655256, acc.: 63.09%] [G loss: 0.755486]\n",
      "215 [D loss: 0.541047, acc.: 74.80%] [G loss: 0.720984]\n",
      "216 [D loss: 0.526016, acc.: 74.41%] [G loss: 0.772812]\n",
      "217 [D loss: 0.511535, acc.: 76.37%] [G loss: 0.823653]\n",
      "218 [D loss: 0.494448, acc.: 77.34%] [G loss: 0.863826]\n",
      "219 [D loss: 0.637291, acc.: 64.26%] [G loss: 0.995511]\n",
      "220 [D loss: 0.880795, acc.: 43.95%] [G loss: 1.087620]\n",
      "221 [D loss: 0.784050, acc.: 49.61%] [G loss: 1.148248]\n",
      "222 [D loss: 0.918179, acc.: 39.26%] [G loss: 1.188791]\n",
      "223 [D loss: 0.905112, acc.: 40.23%] [G loss: 1.129684]\n",
      "224 [D loss: 0.951939, acc.: 37.70%] [G loss: 1.231827]\n",
      "225 [D loss: 0.847056, acc.: 43.95%] [G loss: 1.165955]\n",
      "226 [D loss: 0.784298, acc.: 48.24%] [G loss: 1.081121]\n",
      "227 [D loss: 0.704551, acc.: 58.40%] [G loss: 1.056030]\n",
      "228 [D loss: 0.617451, acc.: 65.43%] [G loss: 1.028204]\n",
      "229 [D loss: 0.703569, acc.: 56.64%] [G loss: 0.950853]\n",
      "230 [D loss: 0.628565, acc.: 65.82%] [G loss: 0.896589]\n",
      "231 [D loss: 0.736332, acc.: 53.12%] [G loss: 0.922295]\n",
      "232 [D loss: 0.674401, acc.: 61.33%] [G loss: 0.964469]\n",
      "233 [D loss: 0.723763, acc.: 56.25%] [G loss: 0.919241]\n",
      "234 [D loss: 0.710683, acc.: 56.45%] [G loss: 0.891354]\n",
      "235 [D loss: 0.816788, acc.: 49.02%] [G loss: 0.928883]\n",
      "236 [D loss: 0.782111, acc.: 52.15%] [G loss: 0.933753]\n",
      "237 [D loss: 0.735000, acc.: 52.73%] [G loss: 1.046129]\n",
      "238 [D loss: 0.712489, acc.: 57.42%] [G loss: 1.063294]\n",
      "239 [D loss: 0.625916, acc.: 66.80%] [G loss: 1.053454]\n",
      "240 [D loss: 0.609766, acc.: 66.80%] [G loss: 0.982569]\n",
      "241 [D loss: 0.599813, acc.: 68.36%] [G loss: 0.921347]\n",
      "242 [D loss: 0.653678, acc.: 61.52%] [G loss: 0.908824]\n",
      "243 [D loss: 0.601239, acc.: 65.43%] [G loss: 0.960683]\n",
      "244 [D loss: 0.578790, acc.: 68.95%] [G loss: 0.939457]\n",
      "245 [D loss: 0.515270, acc.: 75.98%] [G loss: 0.825599]\n",
      "246 [D loss: 0.517643, acc.: 75.00%] [G loss: 0.819483]\n",
      "247 [D loss: 0.533163, acc.: 73.44%] [G loss: 0.879372]\n",
      "248 [D loss: 0.579594, acc.: 72.27%] [G loss: 0.969264]\n",
      "249 [D loss: 0.605278, acc.: 66.60%] [G loss: 1.063865]\n",
      "250 [D loss: 0.639983, acc.: 64.06%] [G loss: 1.130085]\n",
      "251 [D loss: 0.667684, acc.: 62.50%] [G loss: 1.148779]\n",
      "252 [D loss: 0.794152, acc.: 51.17%] [G loss: 1.237914]\n",
      "253 [D loss: 0.806835, acc.: 48.63%] [G loss: 1.272080]\n",
      "254 [D loss: 0.729494, acc.: 57.23%] [G loss: 1.271335]\n",
      "255 [D loss: 0.722155, acc.: 54.30%] [G loss: 1.105126]\n",
      "256 [D loss: 0.616696, acc.: 65.43%] [G loss: 1.100566]\n",
      "257 [D loss: 0.609765, acc.: 66.41%] [G loss: 1.034460]\n",
      "258 [D loss: 0.626894, acc.: 65.43%] [G loss: 0.991255]\n",
      "259 [D loss: 0.677630, acc.: 58.98%] [G loss: 1.011371]\n",
      "260 [D loss: 0.686292, acc.: 60.16%] [G loss: 1.051980]\n",
      "261 [D loss: 0.644425, acc.: 62.89%] [G loss: 1.009345]\n",
      "262 [D loss: 0.811828, acc.: 48.24%] [G loss: 0.922387]\n",
      "263 [D loss: 0.718850, acc.: 55.27%] [G loss: 1.057388]\n",
      "264 [D loss: 0.738803, acc.: 55.08%] [G loss: 0.935215]\n",
      "265 [D loss: 0.645441, acc.: 62.30%] [G loss: 0.893032]\n",
      "266 [D loss: 0.784744, acc.: 51.95%] [G loss: 0.985066]\n",
      "267 [D loss: 0.717747, acc.: 54.88%] [G loss: 1.000067]\n",
      "268 [D loss: 0.830287, acc.: 44.92%] [G loss: 1.011801]\n",
      "269 [D loss: 0.798568, acc.: 49.22%] [G loss: 0.994265]\n",
      "270 [D loss: 0.781760, acc.: 48.24%] [G loss: 0.971148]\n",
      "271 [D loss: 0.690948, acc.: 57.81%] [G loss: 0.968170]\n",
      "272 [D loss: 0.634810, acc.: 64.26%] [G loss: 0.964821]\n",
      "273 [D loss: 0.497130, acc.: 78.12%] [G loss: 0.990141]\n",
      "274 [D loss: 0.501346, acc.: 76.76%] [G loss: 0.967310]\n",
      "275 [D loss: 0.519150, acc.: 74.80%] [G loss: 0.849544]\n",
      "276 [D loss: 0.472947, acc.: 80.27%] [G loss: 0.952881]\n",
      "277 [D loss: 0.494746, acc.: 78.52%] [G loss: 1.138703]\n",
      "278 [D loss: 0.696895, acc.: 58.01%] [G loss: 1.070722]\n",
      "279 [D loss: 0.664888, acc.: 61.72%] [G loss: 1.097764]\n",
      "280 [D loss: 0.712661, acc.: 59.18%] [G loss: 1.127227]\n",
      "281 [D loss: 0.616500, acc.: 65.23%] [G loss: 1.097725]\n",
      "282 [D loss: 0.627117, acc.: 64.65%] [G loss: 0.983961]\n",
      "283 [D loss: 0.601259, acc.: 67.58%] [G loss: 0.921289]\n",
      "284 [D loss: 0.505321, acc.: 75.78%] [G loss: 0.896437]\n",
      "285 [D loss: 0.500678, acc.: 74.80%] [G loss: 0.920721]\n",
      "286 [D loss: 0.646418, acc.: 65.23%] [G loss: 0.998826]\n",
      "287 [D loss: 0.944218, acc.: 38.09%] [G loss: 1.030487]\n",
      "288 [D loss: 0.714040, acc.: 57.23%] [G loss: 1.126533]\n",
      "289 [D loss: 0.711524, acc.: 56.05%] [G loss: 1.190637]\n",
      "290 [D loss: 0.819929, acc.: 46.48%] [G loss: 1.162910]\n",
      "291 [D loss: 0.669695, acc.: 60.94%] [G loss: 1.071445]\n",
      "292 [D loss: 0.698502, acc.: 58.40%] [G loss: 0.976650]\n",
      "293 [D loss: 0.629768, acc.: 64.26%] [G loss: 1.035213]\n",
      "294 [D loss: 0.640545, acc.: 64.65%] [G loss: 0.923525]\n",
      "295 [D loss: 0.640387, acc.: 64.65%] [G loss: 0.908083]\n",
      "296 [D loss: 0.626178, acc.: 69.14%] [G loss: 0.902226]\n",
      "297 [D loss: 0.537150, acc.: 74.22%] [G loss: 0.977971]\n",
      "298 [D loss: 0.607122, acc.: 66.80%] [G loss: 0.882043]\n",
      "299 [D loss: 0.639413, acc.: 62.89%] [G loss: 0.924463]\n",
      "300 [D loss: 0.582477, acc.: 68.16%] [G loss: 0.981077]\n",
      "301 [D loss: 0.590623, acc.: 69.92%] [G loss: 0.964971]\n",
      "302 [D loss: 0.503953, acc.: 76.76%] [G loss: 1.039158]\n",
      "303 [D loss: 0.512856, acc.: 75.20%] [G loss: 0.936636]\n",
      "304 [D loss: 0.491228, acc.: 79.10%] [G loss: 0.970776]\n",
      "305 [D loss: 0.463452, acc.: 79.49%] [G loss: 0.899826]\n",
      "306 [D loss: 0.408508, acc.: 87.11%] [G loss: 0.888745]\n",
      "307 [D loss: 0.369040, acc.: 89.45%] [G loss: 0.924893]\n",
      "308 [D loss: 0.344040, acc.: 89.45%] [G loss: 1.007461]\n",
      "309 [D loss: 0.369046, acc.: 88.28%] [G loss: 1.012949]\n",
      "310 [D loss: 0.380575, acc.: 86.52%] [G loss: 1.073541]\n",
      "311 [D loss: 0.333795, acc.: 89.65%] [G loss: 1.142879]\n",
      "312 [D loss: 0.479542, acc.: 76.95%] [G loss: 1.130256]\n",
      "313 [D loss: 0.564152, acc.: 69.53%] [G loss: 1.164938]\n",
      "314 [D loss: 0.588395, acc.: 69.73%] [G loss: 1.251282]\n",
      "315 [D loss: 0.612758, acc.: 67.19%] [G loss: 1.222558]\n",
      "316 [D loss: 0.641020, acc.: 64.06%] [G loss: 1.133995]\n",
      "317 [D loss: 0.673350, acc.: 60.55%] [G loss: 1.219985]\n",
      "318 [D loss: 0.922086, acc.: 39.65%] [G loss: 1.182770]\n",
      "319 [D loss: 1.015607, acc.: 37.11%] [G loss: 1.255192]\n",
      "320 [D loss: 0.926651, acc.: 40.43%] [G loss: 1.313584]\n",
      "321 [D loss: 1.080999, acc.: 30.08%] [G loss: 1.147371]\n",
      "322 [D loss: 0.950033, acc.: 37.89%] [G loss: 1.092401]\n",
      "323 [D loss: 0.984577, acc.: 35.74%] [G loss: 1.038653]\n",
      "324 [D loss: 0.902967, acc.: 39.06%] [G loss: 1.009041]\n",
      "325 [D loss: 0.760556, acc.: 55.27%] [G loss: 0.999981]\n",
      "326 [D loss: 0.927365, acc.: 39.84%] [G loss: 0.907829]\n",
      "327 [D loss: 0.659526, acc.: 60.55%] [G loss: 0.935937]\n",
      "328 [D loss: 0.766268, acc.: 53.91%] [G loss: 0.871300]\n",
      "329 [D loss: 0.526819, acc.: 72.27%] [G loss: 0.789557]\n",
      "330 [D loss: 0.570101, acc.: 70.31%] [G loss: 0.807127]\n",
      "331 [D loss: 0.602410, acc.: 68.16%] [G loss: 0.817830]\n",
      "332 [D loss: 0.514555, acc.: 77.73%] [G loss: 0.871356]\n",
      "333 [D loss: 0.558425, acc.: 71.68%] [G loss: 0.900114]\n",
      "334 [D loss: 0.461118, acc.: 81.64%] [G loss: 0.942024]\n",
      "335 [D loss: 0.494259, acc.: 77.54%] [G loss: 0.945694]\n",
      "336 [D loss: 0.654716, acc.: 63.48%] [G loss: 0.946186]\n",
      "337 [D loss: 0.555075, acc.: 73.63%] [G loss: 1.068108]\n",
      "338 [D loss: 0.725198, acc.: 56.84%] [G loss: 1.054792]\n",
      "339 [D loss: 0.620652, acc.: 65.82%] [G loss: 1.115271]\n",
      "340 [D loss: 0.565836, acc.: 72.07%] [G loss: 1.143454]\n",
      "341 [D loss: 0.667002, acc.: 59.18%] [G loss: 1.090542]\n",
      "342 [D loss: 0.770422, acc.: 51.37%] [G loss: 1.127562]\n",
      "343 [D loss: 0.566904, acc.: 70.12%] [G loss: 1.118133]\n",
      "344 [D loss: 0.883210, acc.: 42.19%] [G loss: 1.050365]\n",
      "345 [D loss: 0.598628, acc.: 66.80%] [G loss: 1.027742]\n",
      "346 [D loss: 0.560285, acc.: 69.53%] [G loss: 0.946988]\n",
      "347 [D loss: 0.418003, acc.: 83.59%] [G loss: 0.943408]\n",
      "348 [D loss: 0.405389, acc.: 84.57%] [G loss: 0.878795]\n",
      "349 [D loss: 0.378795, acc.: 85.74%] [G loss: 0.887106]\n",
      "350 [D loss: 0.508118, acc.: 76.17%] [G loss: 0.803546]\n",
      "351 [D loss: 0.425165, acc.: 82.03%] [G loss: 0.911283]\n",
      "352 [D loss: 0.567073, acc.: 69.92%] [G loss: 0.903544]\n",
      "353 [D loss: 0.421607, acc.: 84.18%] [G loss: 0.965642]\n",
      "354 [D loss: 0.558115, acc.: 72.46%] [G loss: 1.147532]\n",
      "355 [D loss: 0.538750, acc.: 73.63%] [G loss: 1.101927]\n",
      "356 [D loss: 0.751771, acc.: 53.52%] [G loss: 1.074025]\n",
      "357 [D loss: 0.406096, acc.: 83.59%] [G loss: 1.143570]\n",
      "358 [D loss: 0.577070, acc.: 71.48%] [G loss: 1.071591]\n",
      "359 [D loss: 0.393084, acc.: 83.79%] [G loss: 1.110490]\n",
      "360 [D loss: 0.424916, acc.: 85.16%] [G loss: 1.069885]\n",
      "361 [D loss: 0.404020, acc.: 83.01%] [G loss: 1.018996]\n",
      "362 [D loss: 0.328913, acc.: 88.48%] [G loss: 0.959341]\n",
      "363 [D loss: 0.409168, acc.: 83.20%] [G loss: 0.934938]\n",
      "364 [D loss: 0.602871, acc.: 66.60%] [G loss: 0.968645]\n",
      "365 [D loss: 0.678059, acc.: 59.18%] [G loss: 0.954664]\n",
      "366 [D loss: 0.751171, acc.: 52.93%] [G loss: 1.063373]\n",
      "367 [D loss: 0.757947, acc.: 51.17%] [G loss: 1.119722]\n",
      "368 [D loss: 0.900382, acc.: 43.55%] [G loss: 1.131789]\n",
      "369 [D loss: 0.494761, acc.: 78.71%] [G loss: 1.248802]\n",
      "370 [D loss: 0.620947, acc.: 67.38%] [G loss: 1.170888]\n",
      "371 [D loss: 0.559078, acc.: 71.09%] [G loss: 1.164871]\n",
      "372 [D loss: 0.462028, acc.: 80.47%] [G loss: 1.158087]\n",
      "373 [D loss: 0.490360, acc.: 77.73%] [G loss: 1.085358]\n",
      "374 [D loss: 0.555534, acc.: 72.07%] [G loss: 1.095782]\n",
      "375 [D loss: 0.435912, acc.: 81.64%] [G loss: 0.974628]\n",
      "376 [D loss: 0.503213, acc.: 76.56%] [G loss: 1.025032]\n",
      "377 [D loss: 0.416179, acc.: 83.40%] [G loss: 0.876897]\n",
      "378 [D loss: 0.511147, acc.: 75.98%] [G loss: 0.889320]\n",
      "379 [D loss: 0.434943, acc.: 81.64%] [G loss: 0.927569]\n",
      "380 [D loss: 0.306887, acc.: 91.60%] [G loss: 0.903735]\n",
      "381 [D loss: 0.502871, acc.: 75.39%] [G loss: 0.854880]\n",
      "382 [D loss: 0.382031, acc.: 87.11%] [G loss: 0.892233]\n",
      "383 [D loss: 0.593797, acc.: 68.55%] [G loss: 0.942157]\n",
      "384 [D loss: 0.737442, acc.: 58.40%] [G loss: 1.067482]\n",
      "385 [D loss: 0.745928, acc.: 53.71%] [G loss: 0.995926]\n",
      "386 [D loss: 0.770038, acc.: 52.73%] [G loss: 1.084683]\n",
      "387 [D loss: 0.584768, acc.: 69.92%] [G loss: 1.052493]\n",
      "388 [D loss: 0.616921, acc.: 66.80%] [G loss: 0.949750]\n",
      "389 [D loss: 0.326458, acc.: 89.65%] [G loss: 1.052952]\n",
      "390 [D loss: 0.470711, acc.: 79.30%] [G loss: 0.937349]\n",
      "391 [D loss: 0.534968, acc.: 75.00%] [G loss: 1.001121]\n",
      "392 [D loss: 0.348351, acc.: 88.67%] [G loss: 1.002441]\n",
      "393 [D loss: 0.377984, acc.: 85.16%] [G loss: 0.979554]\n",
      "394 [D loss: 0.322498, acc.: 91.02%] [G loss: 0.949081]\n",
      "395 [D loss: 0.362533, acc.: 86.33%] [G loss: 0.983159]\n",
      "396 [D loss: 0.539711, acc.: 71.68%] [G loss: 0.979761]\n",
      "397 [D loss: 0.333524, acc.: 87.89%] [G loss: 1.107753]\n",
      "398 [D loss: 0.544297, acc.: 72.85%] [G loss: 1.112631]\n",
      "399 [D loss: 0.413884, acc.: 85.55%] [G loss: 1.064313]\n",
      "400 [D loss: 0.778940, acc.: 54.69%] [G loss: 1.229937]\n",
      "401 [D loss: 1.146707, acc.: 25.39%] [G loss: 1.197528]\n",
      "402 [D loss: 0.663890, acc.: 62.50%] [G loss: 1.214657]\n",
      "403 [D loss: 0.887311, acc.: 44.14%] [G loss: 1.107529]\n",
      "404 [D loss: 0.895620, acc.: 44.53%] [G loss: 1.069684]\n",
      "405 [D loss: 0.679144, acc.: 60.16%] [G loss: 1.145906]\n",
      "406 [D loss: 0.509457, acc.: 76.37%] [G loss: 0.969688]\n",
      "407 [D loss: 0.686055, acc.: 61.72%] [G loss: 0.915332]\n",
      "408 [D loss: 0.653175, acc.: 65.82%] [G loss: 1.012192]\n",
      "409 [D loss: 0.511733, acc.: 74.02%] [G loss: 0.940731]\n",
      "410 [D loss: 0.327722, acc.: 89.45%] [G loss: 0.947214]\n",
      "411 [D loss: 0.514129, acc.: 73.44%] [G loss: 0.946294]\n",
      "412 [D loss: 0.678001, acc.: 61.13%] [G loss: 1.028517]\n",
      "413 [D loss: 0.576732, acc.: 69.34%] [G loss: 1.001051]\n",
      "414 [D loss: 0.952625, acc.: 40.04%] [G loss: 1.007556]\n",
      "415 [D loss: 0.782754, acc.: 50.59%] [G loss: 1.072341]\n",
      "416 [D loss: 0.851083, acc.: 47.27%] [G loss: 1.026855]\n",
      "417 [D loss: 0.658052, acc.: 63.48%] [G loss: 1.022655]\n",
      "418 [D loss: 0.548840, acc.: 72.46%] [G loss: 0.953182]\n",
      "419 [D loss: 0.461272, acc.: 80.86%] [G loss: 0.973110]\n",
      "420 [D loss: 0.561570, acc.: 72.27%] [G loss: 0.983512]\n",
      "421 [D loss: 0.447558, acc.: 81.45%] [G loss: 1.035487]\n",
      "422 [D loss: 0.338850, acc.: 88.67%] [G loss: 1.071598]\n",
      "423 [D loss: 0.311238, acc.: 91.41%] [G loss: 0.996025]\n",
      "424 [D loss: 0.396369, acc.: 85.35%] [G loss: 1.016796]\n",
      "425 [D loss: 0.273390, acc.: 93.75%] [G loss: 0.984197]\n",
      "426 [D loss: 0.268078, acc.: 94.34%] [G loss: 0.974710]\n",
      "427 [D loss: 0.290497, acc.: 91.60%] [G loss: 0.953696]\n",
      "428 [D loss: 0.265530, acc.: 92.19%] [G loss: 0.936773]\n",
      "429 [D loss: 0.273253, acc.: 91.99%] [G loss: 0.999586]\n",
      "430 [D loss: 0.318797, acc.: 89.06%] [G loss: 0.918898]\n",
      "431 [D loss: 0.323789, acc.: 90.23%] [G loss: 0.955573]\n",
      "432 [D loss: 0.366802, acc.: 86.91%] [G loss: 0.996336]\n",
      "433 [D loss: 0.365180, acc.: 87.11%] [G loss: 0.995742]\n",
      "434 [D loss: 0.352418, acc.: 88.67%] [G loss: 1.049139]\n",
      "435 [D loss: 0.366624, acc.: 88.09%] [G loss: 1.011112]\n",
      "436 [D loss: 0.518923, acc.: 74.22%] [G loss: 1.091763]\n",
      "437 [D loss: 0.302083, acc.: 91.41%] [G loss: 1.140501]\n",
      "438 [D loss: 0.466713, acc.: 78.12%] [G loss: 1.112522]\n",
      "439 [D loss: 0.293515, acc.: 91.60%] [G loss: 1.149550]\n",
      "440 [D loss: 0.251978, acc.: 93.36%] [G loss: 1.077175]\n",
      "441 [D loss: 0.186183, acc.: 97.46%] [G loss: 1.047864]\n",
      "442 [D loss: 0.246556, acc.: 94.92%] [G loss: 1.088074]\n",
      "443 [D loss: 0.372639, acc.: 84.57%] [G loss: 1.062652]\n",
      "444 [D loss: 0.494371, acc.: 77.54%] [G loss: 1.097149]\n",
      "445 [D loss: 0.368937, acc.: 87.11%] [G loss: 1.072896]\n",
      "446 [D loss: 0.406390, acc.: 83.59%] [G loss: 0.948295]\n",
      "447 [D loss: 0.333419, acc.: 88.48%] [G loss: 0.941427]\n",
      "448 [D loss: 0.242501, acc.: 93.75%] [G loss: 0.962343]\n",
      "449 [D loss: 0.308918, acc.: 89.06%] [G loss: 1.017180]\n",
      "450 [D loss: 0.498675, acc.: 75.00%] [G loss: 1.050579]\n",
      "451 [D loss: 0.298131, acc.: 91.41%] [G loss: 1.085744]\n",
      "452 [D loss: 0.491180, acc.: 75.00%] [G loss: 1.101645]\n",
      "453 [D loss: 0.531781, acc.: 73.83%] [G loss: 1.182914]\n",
      "454 [D loss: 0.558883, acc.: 69.34%] [G loss: 1.235498]\n",
      "455 [D loss: 1.234911, acc.: 28.32%] [G loss: 1.326535]\n",
      "456 [D loss: 1.376642, acc.: 18.36%] [G loss: 1.354414]\n",
      "457 [D loss: 1.096721, acc.: 32.42%] [G loss: 1.195332]\n",
      "458 [D loss: 0.381680, acc.: 85.55%] [G loss: 1.143449]\n",
      "459 [D loss: 0.349092, acc.: 87.50%] [G loss: 0.991474]\n",
      "460 [D loss: 0.199536, acc.: 96.48%] [G loss: 0.964640]\n",
      "461 [D loss: 0.235631, acc.: 95.70%] [G loss: 0.865516]\n",
      "462 [D loss: 0.132015, acc.: 98.63%] [G loss: 0.830811]\n",
      "463 [D loss: 0.097537, acc.: 99.61%] [G loss: 0.838638]\n",
      "464 [D loss: 0.275373, acc.: 92.58%] [G loss: 0.866683]\n",
      "465 [D loss: 0.614612, acc.: 67.19%] [G loss: 0.861236]\n",
      "466 [D loss: 0.450531, acc.: 80.08%] [G loss: 0.984420]\n",
      "467 [D loss: 1.503352, acc.: 22.85%] [G loss: 0.922787]\n",
      "468 [D loss: 0.946903, acc.: 40.62%] [G loss: 1.087426]\n",
      "469 [D loss: 0.724873, acc.: 56.25%] [G loss: 0.956505]\n",
      "470 [D loss: 0.267192, acc.: 92.97%] [G loss: 1.009184]\n",
      "471 [D loss: 0.330654, acc.: 88.67%] [G loss: 0.948579]\n",
      "472 [D loss: 0.227734, acc.: 95.51%] [G loss: 0.933330]\n",
      "473 [D loss: 0.158311, acc.: 98.05%] [G loss: 0.900977]\n",
      "474 [D loss: 0.382002, acc.: 84.96%] [G loss: 1.059168]\n",
      "475 [D loss: 0.472636, acc.: 78.32%] [G loss: 1.066705]\n",
      "476 [D loss: 0.259552, acc.: 92.77%] [G loss: 1.098560]\n",
      "477 [D loss: 0.637223, acc.: 65.04%] [G loss: 1.122701]\n",
      "478 [D loss: 0.629681, acc.: 68.75%] [G loss: 1.079421]\n",
      "479 [D loss: 0.273812, acc.: 92.97%] [G loss: 0.938111]\n",
      "480 [D loss: 0.253065, acc.: 93.36%] [G loss: 0.982902]\n",
      "481 [D loss: 0.153456, acc.: 98.63%] [G loss: 1.003772]\n",
      "482 [D loss: 0.170515, acc.: 97.07%] [G loss: 1.014060]\n",
      "483 [D loss: 0.218623, acc.: 95.31%] [G loss: 0.993266]\n",
      "484 [D loss: 0.465110, acc.: 77.34%] [G loss: 0.923835]\n",
      "485 [D loss: 0.329641, acc.: 90.23%] [G loss: 1.043451]\n",
      "486 [D loss: 0.252391, acc.: 95.31%] [G loss: 1.127431]\n",
      "487 [D loss: 0.683308, acc.: 60.35%] [G loss: 1.225408]\n",
      "488 [D loss: 1.468121, acc.: 12.30%] [G loss: 1.207914]\n",
      "489 [D loss: 0.528824, acc.: 73.63%] [G loss: 1.257435]\n",
      "490 [D loss: 0.577057, acc.: 70.51%] [G loss: 1.256453]\n",
      "491 [D loss: 0.381048, acc.: 84.77%] [G loss: 1.188834]\n",
      "492 [D loss: 0.392249, acc.: 85.55%] [G loss: 1.099544]\n",
      "493 [D loss: 0.159600, acc.: 98.05%] [G loss: 1.088039]\n",
      "494 [D loss: 0.370762, acc.: 86.33%] [G loss: 0.968983]\n",
      "495 [D loss: 0.287917, acc.: 90.62%] [G loss: 0.923022]\n",
      "496 [D loss: 0.151635, acc.: 97.85%] [G loss: 0.910193]\n",
      "497 [D loss: 0.276154, acc.: 92.38%] [G loss: 0.918341]\n",
      "498 [D loss: 0.190133, acc.: 97.27%] [G loss: 0.922678]\n",
      "499 [D loss: 0.226294, acc.: 96.48%] [G loss: 0.899756]\n",
      "500 [D loss: 0.168084, acc.: 97.27%] [G loss: 0.884260]\n",
      "501 [D loss: 0.290936, acc.: 91.21%] [G loss: 0.975876]\n",
      "502 [D loss: 0.279605, acc.: 91.99%] [G loss: 0.968411]\n",
      "503 [D loss: 0.441675, acc.: 78.91%] [G loss: 1.042100]\n",
      "504 [D loss: 0.718437, acc.: 58.59%] [G loss: 0.903961]\n",
      "505 [D loss: 0.295104, acc.: 90.62%] [G loss: 0.995726]\n",
      "506 [D loss: 0.572897, acc.: 71.68%] [G loss: 0.943759]\n",
      "507 [D loss: 0.785439, acc.: 55.27%] [G loss: 1.052969]\n",
      "508 [D loss: 0.768900, acc.: 52.34%] [G loss: 1.069847]\n",
      "509 [D loss: 0.423661, acc.: 83.59%] [G loss: 1.037689]\n",
      "510 [D loss: 0.303813, acc.: 90.23%] [G loss: 0.946427]\n",
      "511 [D loss: 0.252268, acc.: 92.77%] [G loss: 0.959848]\n",
      "512 [D loss: 0.353037, acc.: 88.87%] [G loss: 0.950939]\n",
      "513 [D loss: 0.539846, acc.: 72.66%] [G loss: 1.061283]\n",
      "514 [D loss: 0.464247, acc.: 78.71%] [G loss: 0.956957]\n",
      "515 [D loss: 0.228889, acc.: 94.53%] [G loss: 0.980046]\n",
      "516 [D loss: 0.283193, acc.: 91.80%] [G loss: 0.982777]\n",
      "517 [D loss: 0.387177, acc.: 84.96%] [G loss: 0.928229]\n",
      "518 [D loss: 0.510330, acc.: 74.80%] [G loss: 0.893929]\n",
      "519 [D loss: 0.129024, acc.: 98.83%] [G loss: 0.936781]\n",
      "520 [D loss: 0.437958, acc.: 82.42%] [G loss: 0.859982]\n",
      "521 [D loss: 0.245067, acc.: 94.73%] [G loss: 0.952925]\n",
      "522 [D loss: 0.345456, acc.: 88.87%] [G loss: 1.015545]\n",
      "523 [D loss: 0.292352, acc.: 89.06%] [G loss: 1.035869]\n",
      "524 [D loss: 0.356472, acc.: 86.72%] [G loss: 1.033433]\n",
      "525 [D loss: 0.327846, acc.: 89.26%] [G loss: 1.018637]\n",
      "526 [D loss: 0.240083, acc.: 95.51%] [G loss: 1.058653]\n",
      "527 [D loss: 0.422276, acc.: 82.81%] [G loss: 1.148386]\n",
      "528 [D loss: 0.170487, acc.: 98.44%] [G loss: 1.218230]\n",
      "529 [D loss: 0.299860, acc.: 90.82%] [G loss: 1.133711]\n",
      "530 [D loss: 0.228020, acc.: 93.55%] [G loss: 1.084757]\n",
      "531 [D loss: 0.602838, acc.: 65.62%] [G loss: 1.180607]\n",
      "532 [D loss: 1.752637, acc.: 11.52%] [G loss: 1.185009]\n",
      "533 [D loss: 0.943320, acc.: 45.51%] [G loss: 1.074883]\n",
      "534 [D loss: 0.282604, acc.: 91.80%] [G loss: 1.114664]\n",
      "535 [D loss: 0.413885, acc.: 82.81%] [G loss: 1.033527]\n",
      "536 [D loss: 0.258037, acc.: 91.99%] [G loss: 1.144947]\n",
      "537 [D loss: 1.318999, acc.: 23.44%] [G loss: 1.064403]\n",
      "538 [D loss: 0.870172, acc.: 50.39%] [G loss: 0.953410]\n",
      "539 [D loss: 0.348948, acc.: 88.09%] [G loss: 1.019534]\n",
      "540 [D loss: 0.512598, acc.: 76.37%] [G loss: 0.926001]\n",
      "541 [D loss: 0.310615, acc.: 91.02%] [G loss: 0.971229]\n",
      "542 [D loss: 0.425452, acc.: 83.20%] [G loss: 0.949690]\n",
      "543 [D loss: 0.772630, acc.: 56.45%] [G loss: 1.142214]\n",
      "544 [D loss: 2.027441, acc.: 7.62%] [G loss: 1.056585]\n",
      "545 [D loss: 0.392442, acc.: 83.40%] [G loss: 1.216878]\n",
      "546 [D loss: 1.040258, acc.: 36.52%] [G loss: 1.108597]\n",
      "547 [D loss: 0.265146, acc.: 93.36%] [G loss: 1.058232]\n",
      "548 [D loss: 0.719709, acc.: 59.57%] [G loss: 0.853422]\n",
      "549 [D loss: 0.416748, acc.: 80.86%] [G loss: 0.939414]\n",
      "550 [D loss: 0.470882, acc.: 80.47%] [G loss: 0.812329]\n",
      "551 [D loss: 0.387418, acc.: 82.81%] [G loss: 0.761794]\n",
      "552 [D loss: 0.162485, acc.: 97.27%] [G loss: 0.699083]\n",
      "553 [D loss: 0.131554, acc.: 99.02%] [G loss: 0.737341]\n",
      "554 [D loss: 0.093601, acc.: 99.41%] [G loss: 0.725596]\n",
      "555 [D loss: 0.122048, acc.: 99.41%] [G loss: 0.739196]\n",
      "556 [D loss: 0.126091, acc.: 99.41%] [G loss: 0.754918]\n",
      "557 [D loss: 0.127735, acc.: 98.83%] [G loss: 0.779785]\n",
      "558 [D loss: 0.217712, acc.: 95.51%] [G loss: 0.828363]\n",
      "559 [D loss: 0.264780, acc.: 95.12%] [G loss: 0.898677]\n",
      "560 [D loss: 0.137334, acc.: 98.24%] [G loss: 1.002162]\n",
      "561 [D loss: 0.122551, acc.: 99.41%] [G loss: 0.986779]\n",
      "562 [D loss: 0.137255, acc.: 98.24%] [G loss: 0.967464]\n",
      "563 [D loss: 0.067106, acc.: 100.00%] [G loss: 0.905071]\n",
      "564 [D loss: 0.064331, acc.: 100.00%] [G loss: 0.984830]\n",
      "565 [D loss: 0.085724, acc.: 99.61%] [G loss: 0.971685]\n",
      "566 [D loss: 0.075633, acc.: 99.61%] [G loss: 1.012112]\n",
      "567 [D loss: 0.115601, acc.: 99.22%] [G loss: 0.960701]\n",
      "568 [D loss: 0.141679, acc.: 98.44%] [G loss: 0.949115]\n",
      "569 [D loss: 0.178886, acc.: 96.29%] [G loss: 1.072636]\n",
      "570 [D loss: 0.276110, acc.: 92.38%] [G loss: 1.051667]\n",
      "571 [D loss: 0.169467, acc.: 96.88%] [G loss: 1.130798]\n",
      "572 [D loss: 0.319883, acc.: 89.45%] [G loss: 1.134667]\n",
      "573 [D loss: 0.171082, acc.: 97.07%] [G loss: 1.159967]\n",
      "574 [D loss: 0.238634, acc.: 93.16%] [G loss: 1.139214]\n",
      "575 [D loss: 0.591454, acc.: 70.12%] [G loss: 1.007144]\n",
      "576 [D loss: 0.288972, acc.: 90.62%] [G loss: 1.179876]\n",
      "577 [D loss: 0.385273, acc.: 85.35%] [G loss: 1.140984]\n",
      "578 [D loss: 0.110283, acc.: 99.22%] [G loss: 1.243485]\n",
      "579 [D loss: 0.524607, acc.: 75.20%] [G loss: 1.227302]\n",
      "580 [D loss: 0.576084, acc.: 69.92%] [G loss: 0.993553]\n",
      "581 [D loss: 0.142293, acc.: 97.85%] [G loss: 1.183088]\n",
      "582 [D loss: 0.464343, acc.: 78.71%] [G loss: 1.050154]\n",
      "583 [D loss: 0.409885, acc.: 83.59%] [G loss: 1.135348]\n",
      "584 [D loss: 0.455996, acc.: 79.10%] [G loss: 1.089695]\n",
      "585 [D loss: 0.250838, acc.: 93.95%] [G loss: 1.087373]\n",
      "586 [D loss: 0.634103, acc.: 66.02%] [G loss: 1.051591]\n",
      "587 [D loss: 0.754258, acc.: 55.47%] [G loss: 1.151401]\n",
      "588 [D loss: 0.263405, acc.: 92.38%] [G loss: 1.149173]\n",
      "589 [D loss: 1.057688, acc.: 35.55%] [G loss: 1.054503]\n",
      "590 [D loss: 0.229419, acc.: 94.34%] [G loss: 1.297291]\n",
      "591 [D loss: 0.616441, acc.: 66.41%] [G loss: 1.020578]\n",
      "592 [D loss: 0.281665, acc.: 90.82%] [G loss: 1.087899]\n",
      "593 [D loss: 0.394538, acc.: 85.16%] [G loss: 1.059964]\n",
      "594 [D loss: 0.165784, acc.: 98.05%] [G loss: 0.948110]\n",
      "595 [D loss: 0.150164, acc.: 98.63%] [G loss: 0.947366]\n",
      "596 [D loss: 0.132514, acc.: 98.63%] [G loss: 0.936625]\n",
      "597 [D loss: 0.126905, acc.: 98.24%] [G loss: 0.890164]\n",
      "598 [D loss: 0.080928, acc.: 99.80%] [G loss: 0.882263]\n",
      "599 [D loss: 0.106044, acc.: 98.83%] [G loss: 0.830803]\n",
      "600 [D loss: 0.081523, acc.: 99.61%] [G loss: 0.773437]\n",
      "601 [D loss: 0.057756, acc.: 100.00%] [G loss: 0.929722]\n",
      "602 [D loss: 0.098819, acc.: 99.22%] [G loss: 0.979546]\n",
      "603 [D loss: 0.052037, acc.: 99.80%] [G loss: 0.901841]\n",
      "604 [D loss: 0.037230, acc.: 100.00%] [G loss: 0.861471]\n",
      "605 [D loss: 0.035806, acc.: 100.00%] [G loss: 0.841960]\n",
      "606 [D loss: 0.036015, acc.: 100.00%] [G loss: 0.890065]\n",
      "607 [D loss: 0.032807, acc.: 100.00%] [G loss: 0.805109]\n",
      "608 [D loss: 0.048819, acc.: 99.80%] [G loss: 0.839966]\n",
      "609 [D loss: 0.049218, acc.: 100.00%] [G loss: 0.867269]\n",
      "610 [D loss: 0.026569, acc.: 100.00%] [G loss: 0.967518]\n",
      "611 [D loss: 0.042493, acc.: 100.00%] [G loss: 0.974616]\n",
      "612 [D loss: 0.042928, acc.: 99.80%] [G loss: 0.956975]\n",
      "613 [D loss: 0.032776, acc.: 99.80%] [G loss: 0.872627]\n",
      "614 [D loss: 0.021613, acc.: 100.00%] [G loss: 0.973317]\n",
      "615 [D loss: 0.037780, acc.: 100.00%] [G loss: 0.912603]\n",
      "616 [D loss: 0.019954, acc.: 100.00%] [G loss: 0.963692]\n",
      "617 [D loss: 0.041447, acc.: 100.00%] [G loss: 0.860064]\n",
      "618 [D loss: 0.047339, acc.: 99.61%] [G loss: 0.888411]\n",
      "619 [D loss: 0.068144, acc.: 99.80%] [G loss: 0.902460]\n",
      "620 [D loss: 0.052642, acc.: 99.61%] [G loss: 1.022250]\n",
      "621 [D loss: 0.071078, acc.: 99.41%] [G loss: 0.940452]\n",
      "622 [D loss: 0.039418, acc.: 99.80%] [G loss: 0.980909]\n",
      "623 [D loss: 0.074246, acc.: 100.00%] [G loss: 0.942694]\n",
      "624 [D loss: 0.082754, acc.: 99.61%] [G loss: 1.012725]\n",
      "625 [D loss: 0.160114, acc.: 94.92%] [G loss: 1.088944]\n",
      "626 [D loss: 0.173281, acc.: 97.07%] [G loss: 1.062110]\n",
      "627 [D loss: 0.108316, acc.: 98.24%] [G loss: 1.021188]\n",
      "628 [D loss: 0.168913, acc.: 97.07%] [G loss: 1.145281]\n",
      "629 [D loss: 0.151640, acc.: 97.27%] [G loss: 1.247700]\n",
      "630 [D loss: 0.351348, acc.: 85.94%] [G loss: 1.154132]\n",
      "631 [D loss: 0.301099, acc.: 88.09%] [G loss: 1.116325]\n",
      "632 [D loss: 0.168367, acc.: 96.09%] [G loss: 1.248501]\n",
      "633 [D loss: 0.784281, acc.: 56.84%] [G loss: 1.279398]\n",
      "634 [D loss: 1.378892, acc.: 23.63%] [G loss: 1.443734]\n",
      "635 [D loss: 1.161932, acc.: 31.64%] [G loss: 1.385617]\n",
      "636 [D loss: 1.101834, acc.: 36.33%] [G loss: 1.441634]\n",
      "637 [D loss: 0.680120, acc.: 62.89%] [G loss: 1.260718]\n",
      "638 [D loss: 0.260691, acc.: 91.99%] [G loss: 1.207273]\n",
      "639 [D loss: 0.553409, acc.: 73.63%] [G loss: 1.208764]\n",
      "640 [D loss: 0.849425, acc.: 50.59%] [G loss: 1.288847]\n",
      "641 [D loss: 2.333988, acc.: 4.49%] [G loss: 1.266788]\n",
      "642 [D loss: 0.937934, acc.: 44.34%] [G loss: 1.268460]\n",
      "643 [D loss: 1.725450, acc.: 15.62%] [G loss: 1.155657]\n",
      "644 [D loss: 1.156279, acc.: 37.11%] [G loss: 1.011361]\n",
      "645 [D loss: 0.478492, acc.: 77.73%] [G loss: 0.944602]\n",
      "646 [D loss: 0.224068, acc.: 94.34%] [G loss: 0.812633]\n",
      "647 [D loss: 0.092123, acc.: 98.83%] [G loss: 0.801313]\n",
      "648 [D loss: 0.086975, acc.: 99.02%] [G loss: 0.769398]\n",
      "649 [D loss: 0.051956, acc.: 100.00%] [G loss: 0.750762]\n",
      "650 [D loss: 0.086080, acc.: 98.63%] [G loss: 0.729853]\n",
      "651 [D loss: 0.058100, acc.: 99.61%] [G loss: 0.693163]\n",
      "652 [D loss: 0.088907, acc.: 99.02%] [G loss: 0.659344]\n",
      "653 [D loss: 0.060736, acc.: 100.00%] [G loss: 0.699891]\n",
      "654 [D loss: 0.090208, acc.: 99.61%] [G loss: 0.708709]\n",
      "655 [D loss: 0.067590, acc.: 99.41%] [G loss: 0.637754]\n",
      "656 [D loss: 0.041342, acc.: 99.80%] [G loss: 0.752822]\n",
      "657 [D loss: 0.091360, acc.: 99.41%] [G loss: 0.740683]\n",
      "658 [D loss: 0.092880, acc.: 99.61%] [G loss: 0.737822]\n",
      "659 [D loss: 0.060808, acc.: 99.80%] [G loss: 0.831493]\n",
      "660 [D loss: 0.096143, acc.: 99.02%] [G loss: 0.835084]\n",
      "661 [D loss: 0.081969, acc.: 99.61%] [G loss: 0.849172]\n",
      "662 [D loss: 0.059707, acc.: 99.41%] [G loss: 0.886572]\n",
      "663 [D loss: 0.105037, acc.: 99.02%] [G loss: 0.938193]\n",
      "664 [D loss: 0.059220, acc.: 99.80%] [G loss: 0.869865]\n",
      "665 [D loss: 0.144629, acc.: 98.05%] [G loss: 1.036786]\n",
      "666 [D loss: 0.107633, acc.: 98.24%] [G loss: 0.963820]\n",
      "667 [D loss: 0.073022, acc.: 99.80%] [G loss: 1.120629]\n",
      "668 [D loss: 0.101899, acc.: 99.41%] [G loss: 1.193096]\n",
      "669 [D loss: 0.083059, acc.: 99.41%] [G loss: 1.124724]\n",
      "670 [D loss: 0.138052, acc.: 98.44%] [G loss: 1.108124]\n",
      "671 [D loss: 0.112392, acc.: 98.83%] [G loss: 1.252028]\n",
      "672 [D loss: 0.061149, acc.: 100.00%] [G loss: 1.231384]\n",
      "673 [D loss: 0.061367, acc.: 99.22%] [G loss: 1.134743]\n",
      "674 [D loss: 0.075524, acc.: 99.80%] [G loss: 1.112677]\n",
      "675 [D loss: 0.072158, acc.: 100.00%] [G loss: 1.100538]\n",
      "676 [D loss: 0.086819, acc.: 99.41%] [G loss: 1.169289]\n",
      "677 [D loss: 0.127001, acc.: 98.24%] [G loss: 1.143489]\n",
      "678 [D loss: 0.327307, acc.: 88.48%] [G loss: 1.236687]\n",
      "679 [D loss: 0.093087, acc.: 99.22%] [G loss: 1.253181]\n",
      "680 [D loss: 0.205026, acc.: 94.53%] [G loss: 1.213921]\n",
      "681 [D loss: 0.322314, acc.: 89.06%] [G loss: 1.256784]\n",
      "682 [D loss: 0.126387, acc.: 98.63%] [G loss: 1.288882]\n",
      "683 [D loss: 0.117229, acc.: 99.02%] [G loss: 1.170276]\n",
      "684 [D loss: 0.095775, acc.: 98.83%] [G loss: 1.194852]\n",
      "685 [D loss: 0.097272, acc.: 99.41%] [G loss: 1.158982]\n",
      "686 [D loss: 0.070132, acc.: 99.22%] [G loss: 1.109306]\n",
      "687 [D loss: 0.050836, acc.: 99.80%] [G loss: 1.072302]\n",
      "688 [D loss: 0.145725, acc.: 98.24%] [G loss: 0.951799]\n",
      "689 [D loss: 0.061307, acc.: 99.41%] [G loss: 1.054980]\n",
      "690 [D loss: 0.336054, acc.: 87.11%] [G loss: 1.086064]\n",
      "691 [D loss: 0.185763, acc.: 94.53%] [G loss: 1.150134]\n",
      "692 [D loss: 0.263748, acc.: 90.82%] [G loss: 1.147022]\n",
      "693 [D loss: 0.884742, acc.: 48.83%] [G loss: 1.181304]\n",
      "694 [D loss: 0.428415, acc.: 78.71%] [G loss: 1.030658]\n",
      "695 [D loss: 0.154959, acc.: 94.14%] [G loss: 1.134994]\n",
      "696 [D loss: 0.161556, acc.: 94.92%] [G loss: 1.215081]\n",
      "697 [D loss: 0.244823, acc.: 90.23%] [G loss: 1.071059]\n",
      "698 [D loss: 0.202091, acc.: 94.14%] [G loss: 1.162971]\n",
      "699 [D loss: 0.136051, acc.: 96.48%] [G loss: 0.968727]\n",
      "700 [D loss: 0.094823, acc.: 98.83%] [G loss: 1.196094]\n",
      "701 [D loss: 0.178630, acc.: 95.31%] [G loss: 1.081298]\n",
      "702 [D loss: 0.075775, acc.: 99.61%] [G loss: 1.119343]\n",
      "703 [D loss: 0.195379, acc.: 96.09%] [G loss: 1.101766]\n",
      "704 [D loss: 0.098585, acc.: 98.83%] [G loss: 1.097988]\n",
      "705 [D loss: 0.169809, acc.: 96.48%] [G loss: 1.040987]\n",
      "706 [D loss: 0.254332, acc.: 91.99%] [G loss: 1.169322]\n",
      "707 [D loss: 0.132516, acc.: 98.05%] [G loss: 1.344087]\n",
      "708 [D loss: 0.091817, acc.: 99.41%] [G loss: 1.144152]\n",
      "709 [D loss: 0.121475, acc.: 98.24%] [G loss: 1.129982]\n",
      "710 [D loss: 0.073731, acc.: 98.63%] [G loss: 1.121577]\n",
      "711 [D loss: 0.135777, acc.: 98.44%] [G loss: 1.046124]\n",
      "712 [D loss: 0.131474, acc.: 98.24%] [G loss: 1.028110]\n",
      "713 [D loss: 0.043111, acc.: 99.80%] [G loss: 1.126838]\n",
      "714 [D loss: 0.412623, acc.: 81.84%] [G loss: 1.153632]\n",
      "715 [D loss: 0.147614, acc.: 96.09%] [G loss: 1.446950]\n",
      "716 [D loss: 0.088533, acc.: 99.02%] [G loss: 1.186683]\n",
      "717 [D loss: 0.049662, acc.: 100.00%] [G loss: 1.321817]\n",
      "718 [D loss: 0.035617, acc.: 100.00%] [G loss: 1.153241]\n",
      "719 [D loss: 0.033825, acc.: 100.00%] [G loss: 1.135735]\n",
      "720 [D loss: 0.018565, acc.: 100.00%] [G loss: 1.117501]\n",
      "721 [D loss: 0.054455, acc.: 99.41%] [G loss: 0.935993]\n",
      "722 [D loss: 0.033027, acc.: 99.80%] [G loss: 1.250469]\n",
      "723 [D loss: 0.037554, acc.: 99.80%] [G loss: 1.025232]\n",
      "724 [D loss: 0.020963, acc.: 100.00%] [G loss: 0.983290]\n",
      "725 [D loss: 0.030949, acc.: 100.00%] [G loss: 0.928262]\n",
      "726 [D loss: 0.047720, acc.: 100.00%] [G loss: 1.029042]\n",
      "727 [D loss: 0.034729, acc.: 100.00%] [G loss: 1.060703]\n",
      "728 [D loss: 0.062908, acc.: 98.63%] [G loss: 0.898705]\n",
      "729 [D loss: 0.045712, acc.: 99.80%] [G loss: 1.056790]\n",
      "730 [D loss: 0.046478, acc.: 100.00%] [G loss: 0.991595]\n",
      "731 [D loss: 0.060568, acc.: 99.22%] [G loss: 0.908391]\n",
      "732 [D loss: 0.027901, acc.: 100.00%] [G loss: 1.066444]\n",
      "733 [D loss: 0.096019, acc.: 99.22%] [G loss: 0.988564]\n",
      "734 [D loss: 0.044032, acc.: 99.80%] [G loss: 0.910056]\n",
      "735 [D loss: 0.064901, acc.: 99.80%] [G loss: 0.967520]\n",
      "736 [D loss: 0.081676, acc.: 99.61%] [G loss: 1.002808]\n",
      "737 [D loss: 0.055866, acc.: 99.61%] [G loss: 1.100242]\n",
      "738 [D loss: 0.375349, acc.: 85.55%] [G loss: 0.968394]\n",
      "739 [D loss: 0.166390, acc.: 94.92%] [G loss: 1.140032]\n",
      "740 [D loss: 0.036550, acc.: 100.00%] [G loss: 1.076835]\n",
      "741 [D loss: 0.068495, acc.: 99.61%] [G loss: 1.007771]\n",
      "742 [D loss: 0.030682, acc.: 100.00%] [G loss: 1.103441]\n",
      "743 [D loss: 0.069303, acc.: 99.80%] [G loss: 1.066804]\n",
      "744 [D loss: 0.040627, acc.: 99.41%] [G loss: 1.129295]\n",
      "745 [D loss: 0.053847, acc.: 99.80%] [G loss: 0.976692]\n",
      "746 [D loss: 0.078685, acc.: 99.41%] [G loss: 1.133889]\n",
      "747 [D loss: 0.077758, acc.: 99.41%] [G loss: 1.018928]\n",
      "748 [D loss: 0.032976, acc.: 99.80%] [G loss: 1.234015]\n",
      "749 [D loss: 0.145625, acc.: 97.27%] [G loss: 1.086391]\n",
      "750 [D loss: 0.093391, acc.: 98.83%] [G loss: 1.308218]\n",
      "751 [D loss: 0.119692, acc.: 97.27%] [G loss: 1.149455]\n",
      "752 [D loss: 0.092599, acc.: 99.61%] [G loss: 1.226814]\n",
      "753 [D loss: 0.080018, acc.: 99.80%] [G loss: 1.145886]\n",
      "754 [D loss: 0.065488, acc.: 99.61%] [G loss: 1.080251]\n",
      "755 [D loss: 0.146892, acc.: 97.46%] [G loss: 1.137747]\n",
      "756 [D loss: 0.106466, acc.: 98.63%] [G loss: 1.102306]\n",
      "757 [D loss: 0.145468, acc.: 97.46%] [G loss: 1.130477]\n",
      "758 [D loss: 0.097580, acc.: 99.02%] [G loss: 1.214599]\n",
      "759 [D loss: 0.831377, acc.: 52.15%] [G loss: 1.366797]\n",
      "760 [D loss: 3.852539, acc.: 0.20%] [G loss: 1.333818]\n",
      "761 [D loss: 0.107869, acc.: 97.66%] [G loss: 2.080172]\n",
      "762 [D loss: 2.600280, acc.: 5.08%] [G loss: 1.477859]\n",
      "763 [D loss: 0.069053, acc.: 99.41%] [G loss: 1.731969]\n",
      "764 [D loss: 0.524783, acc.: 71.48%] [G loss: 1.200699]\n",
      "765 [D loss: 0.038135, acc.: 99.80%] [G loss: 0.640289]\n",
      "766 [D loss: 0.004891, acc.: 100.00%] [G loss: 0.599102]\n",
      "767 [D loss: 0.003073, acc.: 100.00%] [G loss: 0.700979]\n",
      "768 [D loss: 0.005389, acc.: 100.00%] [G loss: 0.662243]\n",
      "769 [D loss: 0.005011, acc.: 100.00%] [G loss: 0.707633]\n",
      "770 [D loss: 0.012029, acc.: 100.00%] [G loss: 0.704340]\n",
      "771 [D loss: 0.015541, acc.: 100.00%] [G loss: 0.690117]\n",
      "772 [D loss: 0.011451, acc.: 100.00%] [G loss: 0.786512]\n",
      "773 [D loss: 0.008627, acc.: 100.00%] [G loss: 0.794397]\n",
      "774 [D loss: 0.017261, acc.: 100.00%] [G loss: 0.743038]\n",
      "775 [D loss: 0.018953, acc.: 100.00%] [G loss: 0.732922]\n",
      "776 [D loss: 0.015426, acc.: 100.00%] [G loss: 0.744884]\n",
      "777 [D loss: 0.016670, acc.: 100.00%] [G loss: 0.820185]\n",
      "778 [D loss: 0.023407, acc.: 99.61%] [G loss: 0.817026]\n",
      "779 [D loss: 0.026441, acc.: 100.00%] [G loss: 0.848654]\n",
      "780 [D loss: 0.012459, acc.: 100.00%] [G loss: 0.876406]\n",
      "781 [D loss: 0.015446, acc.: 100.00%] [G loss: 0.910876]\n",
      "782 [D loss: 0.010068, acc.: 100.00%] [G loss: 0.923921]\n",
      "783 [D loss: 0.017263, acc.: 100.00%] [G loss: 0.796844]\n",
      "784 [D loss: 0.015142, acc.: 100.00%] [G loss: 0.874894]\n",
      "785 [D loss: 0.017653, acc.: 100.00%] [G loss: 0.843572]\n",
      "786 [D loss: 0.013140, acc.: 100.00%] [G loss: 0.827107]\n",
      "787 [D loss: 0.009032, acc.: 100.00%] [G loss: 0.983648]\n",
      "788 [D loss: 0.009326, acc.: 100.00%] [G loss: 0.889762]\n",
      "789 [D loss: 0.013066, acc.: 100.00%] [G loss: 0.823598]\n",
      "790 [D loss: 0.010971, acc.: 100.00%] [G loss: 0.793722]\n",
      "791 [D loss: 0.008931, acc.: 100.00%] [G loss: 0.826294]\n",
      "792 [D loss: 0.012696, acc.: 100.00%] [G loss: 0.779022]\n",
      "793 [D loss: 0.007176, acc.: 100.00%] [G loss: 0.904288]\n",
      "794 [D loss: 0.009724, acc.: 100.00%] [G loss: 0.887730]\n",
      "795 [D loss: 0.005702, acc.: 100.00%] [G loss: 0.892725]\n",
      "796 [D loss: 0.007978, acc.: 100.00%] [G loss: 0.853452]\n",
      "797 [D loss: 0.006975, acc.: 100.00%] [G loss: 0.729610]\n",
      "798 [D loss: 0.010749, acc.: 100.00%] [G loss: 0.763861]\n",
      "799 [D loss: 0.006391, acc.: 100.00%] [G loss: 0.845347]\n",
      "800 [D loss: 0.005646, acc.: 100.00%] [G loss: 0.824077]\n",
      "801 [D loss: 0.003718, acc.: 100.00%] [G loss: 0.783925]\n",
      "802 [D loss: 0.003130, acc.: 100.00%] [G loss: 0.853506]\n",
      "803 [D loss: 0.004021, acc.: 100.00%] [G loss: 0.962023]\n",
      "804 [D loss: 0.004243, acc.: 100.00%] [G loss: 0.931415]\n",
      "805 [D loss: 0.002451, acc.: 100.00%] [G loss: 0.965787]\n",
      "806 [D loss: 0.003053, acc.: 100.00%] [G loss: 1.121480]\n",
      "807 [D loss: 0.002095, acc.: 100.00%] [G loss: 0.959211]\n",
      "808 [D loss: 0.003793, acc.: 100.00%] [G loss: 1.089176]\n",
      "809 [D loss: 0.001542, acc.: 100.00%] [G loss: 1.021469]\n",
      "810 [D loss: 0.002277, acc.: 100.00%] [G loss: 1.183490]\n",
      "811 [D loss: 0.002681, acc.: 100.00%] [G loss: 1.377638]\n",
      "812 [D loss: 0.001126, acc.: 100.00%] [G loss: 1.189829]\n",
      "813 [D loss: 0.002083, acc.: 100.00%] [G loss: 1.228633]\n",
      "814 [D loss: 0.001764, acc.: 100.00%] [G loss: 1.103619]\n",
      "815 [D loss: 0.001447, acc.: 100.00%] [G loss: 0.947904]\n",
      "816 [D loss: 0.001220, acc.: 100.00%] [G loss: 0.934836]\n",
      "817 [D loss: 0.002019, acc.: 100.00%] [G loss: 0.940925]\n",
      "818 [D loss: 0.001919, acc.: 100.00%] [G loss: 0.860732]\n",
      "819 [D loss: 0.001447, acc.: 100.00%] [G loss: 0.915721]\n",
      "820 [D loss: 0.001171, acc.: 100.00%] [G loss: 0.763497]\n",
      "821 [D loss: 0.000877, acc.: 100.00%] [G loss: 0.772649]\n",
      "822 [D loss: 0.001372, acc.: 100.00%] [G loss: 0.750468]\n",
      "823 [D loss: 0.002540, acc.: 100.00%] [G loss: 0.689903]\n",
      "824 [D loss: 0.001798, acc.: 100.00%] [G loss: 0.685181]\n",
      "825 [D loss: 0.001756, acc.: 100.00%] [G loss: 0.694823]\n",
      "826 [D loss: 0.001832, acc.: 100.00%] [G loss: 0.680065]\n",
      "827 [D loss: 0.001670, acc.: 100.00%] [G loss: 0.549302]\n",
      "828 [D loss: 0.001703, acc.: 100.00%] [G loss: 0.652392]\n",
      "829 [D loss: 0.002168, acc.: 100.00%] [G loss: 0.581933]\n",
      "830 [D loss: 0.002454, acc.: 100.00%] [G loss: 0.576973]\n",
      "831 [D loss: 0.001708, acc.: 100.00%] [G loss: 0.566579]\n",
      "832 [D loss: 0.002057, acc.: 100.00%] [G loss: 0.531995]\n",
      "833 [D loss: 0.002311, acc.: 100.00%] [G loss: 0.472708]\n",
      "834 [D loss: 0.001153, acc.: 100.00%] [G loss: 0.528767]\n",
      "835 [D loss: 0.001958, acc.: 100.00%] [G loss: 0.459513]\n",
      "836 [D loss: 0.001778, acc.: 100.00%] [G loss: 0.422384]\n",
      "837 [D loss: 0.002962, acc.: 100.00%] [G loss: 0.498196]\n",
      "838 [D loss: 0.001508, acc.: 100.00%] [G loss: 0.492304]\n",
      "839 [D loss: 0.001423, acc.: 100.00%] [G loss: 0.548074]\n",
      "840 [D loss: 0.001970, acc.: 100.00%] [G loss: 0.480948]\n",
      "841 [D loss: 0.002385, acc.: 100.00%] [G loss: 0.500164]\n",
      "842 [D loss: 0.001988, acc.: 100.00%] [G loss: 0.515586]\n",
      "843 [D loss: 0.001684, acc.: 100.00%] [G loss: 0.519654]\n",
      "844 [D loss: 0.002655, acc.: 100.00%] [G loss: 0.530321]\n",
      "845 [D loss: 0.001212, acc.: 100.00%] [G loss: 0.571603]\n",
      "846 [D loss: 0.001005, acc.: 100.00%] [G loss: 0.591779]\n",
      "847 [D loss: 0.001493, acc.: 100.00%] [G loss: 0.637264]\n",
      "848 [D loss: 0.000753, acc.: 100.00%] [G loss: 0.657530]\n",
      "849 [D loss: 0.001119, acc.: 100.00%] [G loss: 0.690885]\n",
      "850 [D loss: 0.000978, acc.: 100.00%] [G loss: 0.605579]\n",
      "851 [D loss: 0.001740, acc.: 100.00%] [G loss: 0.673838]\n",
      "852 [D loss: 0.000846, acc.: 100.00%] [G loss: 0.708914]\n",
      "853 [D loss: 0.001365, acc.: 100.00%] [G loss: 0.657699]\n",
      "854 [D loss: 0.001111, acc.: 100.00%] [G loss: 0.640840]\n",
      "855 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.684937]\n",
      "856 [D loss: 0.001271, acc.: 100.00%] [G loss: 0.612498]\n",
      "857 [D loss: 0.001276, acc.: 100.00%] [G loss: 0.551308]\n",
      "858 [D loss: 0.001228, acc.: 100.00%] [G loss: 0.584773]\n",
      "859 [D loss: 0.001551, acc.: 100.00%] [G loss: 0.564235]\n",
      "860 [D loss: 0.001191, acc.: 100.00%] [G loss: 0.642636]\n",
      "861 [D loss: 0.001228, acc.: 100.00%] [G loss: 0.632581]\n",
      "862 [D loss: 0.000926, acc.: 100.00%] [G loss: 0.587573]\n",
      "863 [D loss: 0.001685, acc.: 100.00%] [G loss: 0.597501]\n",
      "864 [D loss: 0.001270, acc.: 100.00%] [G loss: 0.674758]\n",
      "865 [D loss: 0.001232, acc.: 100.00%] [G loss: 0.649025]\n",
      "866 [D loss: 0.001057, acc.: 100.00%] [G loss: 0.707910]\n",
      "867 [D loss: 0.000659, acc.: 100.00%] [G loss: 0.662585]\n",
      "868 [D loss: 0.001013, acc.: 100.00%] [G loss: 0.661494]\n",
      "869 [D loss: 0.001050, acc.: 100.00%] [G loss: 0.696581]\n",
      "870 [D loss: 0.001472, acc.: 100.00%] [G loss: 0.709656]\n",
      "871 [D loss: 0.001370, acc.: 100.00%] [G loss: 0.619633]\n",
      "872 [D loss: 0.001743, acc.: 100.00%] [G loss: 0.614968]\n",
      "873 [D loss: 0.002422, acc.: 100.00%] [G loss: 0.522041]\n",
      "874 [D loss: 0.001414, acc.: 100.00%] [G loss: 0.624812]\n",
      "875 [D loss: 0.001615, acc.: 100.00%] [G loss: 0.700953]\n",
      "876 [D loss: 0.000798, acc.: 100.00%] [G loss: 0.704402]\n",
      "877 [D loss: 0.000973, acc.: 100.00%] [G loss: 0.708032]\n",
      "878 [D loss: 0.000929, acc.: 100.00%] [G loss: 0.687633]\n",
      "879 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.721280]\n",
      "880 [D loss: 0.000662, acc.: 100.00%] [G loss: 0.727544]\n",
      "881 [D loss: 0.000582, acc.: 100.00%] [G loss: 0.771699]\n",
      "882 [D loss: 0.000714, acc.: 100.00%] [G loss: 0.785891]\n",
      "883 [D loss: 0.000864, acc.: 100.00%] [G loss: 0.788518]\n",
      "884 [D loss: 0.000840, acc.: 100.00%] [G loss: 0.740849]\n",
      "885 [D loss: 0.000768, acc.: 100.00%] [G loss: 0.656183]\n",
      "886 [D loss: 0.000792, acc.: 100.00%] [G loss: 0.685788]\n",
      "887 [D loss: 0.000826, acc.: 100.00%] [G loss: 0.747775]\n",
      "888 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.671867]\n",
      "889 [D loss: 0.000884, acc.: 100.00%] [G loss: 0.703622]\n",
      "890 [D loss: 0.000756, acc.: 100.00%] [G loss: 0.746317]\n",
      "891 [D loss: 0.000731, acc.: 100.00%] [G loss: 0.771577]\n",
      "892 [D loss: 0.001159, acc.: 100.00%] [G loss: 0.714426]\n",
      "893 [D loss: 0.000982, acc.: 100.00%] [G loss: 0.771335]\n",
      "894 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.780210]\n",
      "895 [D loss: 0.000584, acc.: 100.00%] [G loss: 0.747679]\n",
      "896 [D loss: 0.000909, acc.: 100.00%] [G loss: 0.767669]\n",
      "897 [D loss: 0.001008, acc.: 100.00%] [G loss: 0.751335]\n",
      "898 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.811424]\n",
      "899 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.868954]\n",
      "900 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.932270]\n",
      "901 [D loss: 0.001264, acc.: 100.00%] [G loss: 0.873687]\n",
      "902 [D loss: 0.000914, acc.: 100.00%] [G loss: 0.952805]\n",
      "903 [D loss: 0.000486, acc.: 100.00%] [G loss: 0.812833]\n",
      "904 [D loss: 0.000522, acc.: 100.00%] [G loss: 0.905039]\n",
      "905 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.806787]\n",
      "906 [D loss: 0.000551, acc.: 100.00%] [G loss: 0.755511]\n",
      "907 [D loss: 0.000607, acc.: 100.00%] [G loss: 0.802845]\n",
      "908 [D loss: 0.000750, acc.: 100.00%] [G loss: 0.761394]\n",
      "909 [D loss: 0.000845, acc.: 100.00%] [G loss: 0.934083]\n",
      "910 [D loss: 0.000636, acc.: 100.00%] [G loss: 0.853303]\n",
      "911 [D loss: 0.000486, acc.: 100.00%] [G loss: 0.867267]\n",
      "912 [D loss: 0.000688, acc.: 100.00%] [G loss: 0.878307]\n",
      "913 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.828748]\n",
      "914 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.897773]\n",
      "915 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.887439]\n",
      "916 [D loss: 0.000466, acc.: 100.00%] [G loss: 0.853251]\n",
      "917 [D loss: 0.000663, acc.: 100.00%] [G loss: 0.834253]\n",
      "918 [D loss: 0.000588, acc.: 100.00%] [G loss: 0.824129]\n",
      "919 [D loss: 0.000416, acc.: 100.00%] [G loss: 0.841030]\n",
      "920 [D loss: 0.000662, acc.: 100.00%] [G loss: 0.904912]\n",
      "921 [D loss: 0.000443, acc.: 100.00%] [G loss: 0.967153]\n",
      "922 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.943031]\n",
      "923 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.930872]\n",
      "924 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.958996]\n",
      "925 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.931287]\n",
      "926 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.968307]\n",
      "927 [D loss: 0.000344, acc.: 100.00%] [G loss: 1.071180]\n",
      "928 [D loss: 0.000496, acc.: 100.00%] [G loss: 1.088147]\n",
      "929 [D loss: 0.000530, acc.: 100.00%] [G loss: 1.044538]\n",
      "930 [D loss: 0.000537, acc.: 100.00%] [G loss: 0.966166]\n",
      "931 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.934127]\n",
      "932 [D loss: 0.000503, acc.: 100.00%] [G loss: 1.018192]\n",
      "933 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.915124]\n",
      "934 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.803489]\n",
      "935 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.850558]\n",
      "936 [D loss: 0.000738, acc.: 100.00%] [G loss: 0.820087]\n",
      "937 [D loss: 0.000758, acc.: 100.00%] [G loss: 0.710633]\n",
      "938 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.749838]\n",
      "939 [D loss: 0.000545, acc.: 100.00%] [G loss: 0.890613]\n",
      "940 [D loss: 0.000455, acc.: 100.00%] [G loss: 0.815906]\n",
      "941 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.968382]\n",
      "942 [D loss: 0.000677, acc.: 100.00%] [G loss: 0.879637]\n",
      "943 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.946659]\n",
      "944 [D loss: 0.000480, acc.: 100.00%] [G loss: 0.863746]\n",
      "945 [D loss: 0.000545, acc.: 100.00%] [G loss: 0.891921]\n",
      "946 [D loss: 0.000472, acc.: 100.00%] [G loss: 0.858624]\n",
      "947 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.793162]\n",
      "948 [D loss: 0.000505, acc.: 100.00%] [G loss: 0.917652]\n",
      "949 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.841513]\n",
      "950 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.875705]\n",
      "951 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.926686]\n",
      "952 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.868393]\n",
      "953 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.871974]\n",
      "954 [D loss: 0.000486, acc.: 100.00%] [G loss: 1.024551]\n",
      "955 [D loss: 0.000287, acc.: 100.00%] [G loss: 1.033010]\n",
      "956 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.988981]\n",
      "957 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.911817]\n",
      "958 [D loss: 0.000266, acc.: 100.00%] [G loss: 1.020407]\n",
      "959 [D loss: 0.000360, acc.: 100.00%] [G loss: 1.160128]\n",
      "960 [D loss: 0.000246, acc.: 100.00%] [G loss: 1.009017]\n",
      "961 [D loss: 0.000261, acc.: 100.00%] [G loss: 1.088143]\n",
      "962 [D loss: 0.000452, acc.: 100.00%] [G loss: 1.079212]\n",
      "963 [D loss: 0.000263, acc.: 100.00%] [G loss: 1.053772]\n",
      "964 [D loss: 0.000381, acc.: 100.00%] [G loss: 1.129473]\n",
      "965 [D loss: 0.000370, acc.: 100.00%] [G loss: 1.247259]\n",
      "966 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.972074]\n",
      "967 [D loss: 0.000236, acc.: 100.00%] [G loss: 1.024863]\n",
      "968 [D loss: 0.000245, acc.: 100.00%] [G loss: 1.025448]\n",
      "969 [D loss: 0.000242, acc.: 100.00%] [G loss: 1.113425]\n",
      "970 [D loss: 0.000298, acc.: 100.00%] [G loss: 1.075184]\n",
      "971 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.862321]\n",
      "972 [D loss: 0.000340, acc.: 100.00%] [G loss: 1.010885]\n",
      "973 [D loss: 0.000412, acc.: 100.00%] [G loss: 0.950210]\n",
      "974 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.936627]\n",
      "975 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.935947]\n",
      "976 [D loss: 0.000384, acc.: 100.00%] [G loss: 1.060418]\n",
      "977 [D loss: 0.000237, acc.: 100.00%] [G loss: 1.035186]\n",
      "978 [D loss: 0.000324, acc.: 100.00%] [G loss: 1.031259]\n",
      "979 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.889451]\n",
      "980 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.842551]\n",
      "981 [D loss: 0.000318, acc.: 100.00%] [G loss: 0.988244]\n",
      "982 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.936420]\n",
      "983 [D loss: 0.000304, acc.: 100.00%] [G loss: 1.010102]\n",
      "984 [D loss: 0.000254, acc.: 100.00%] [G loss: 1.032942]\n",
      "985 [D loss: 0.000369, acc.: 100.00%] [G loss: 1.074304]\n",
      "986 [D loss: 0.000290, acc.: 100.00%] [G loss: 1.013883]\n",
      "987 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.913800]\n",
      "988 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.962091]\n",
      "989 [D loss: 0.000417, acc.: 100.00%] [G loss: 0.916765]\n",
      "990 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.978263]\n",
      "991 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.920663]\n",
      "992 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.818370]\n",
      "993 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.961200]\n",
      "994 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.925124]\n",
      "995 [D loss: 0.000327, acc.: 100.00%] [G loss: 0.970993]\n",
      "996 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.875871]\n",
      "997 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.863010]\n",
      "998 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.927646]\n",
      "999 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.833408]\n"
     ]
    }
   ],
   "source": [
    "dcgan.train(epochs=1000, batch_size=256, save_interval=50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
