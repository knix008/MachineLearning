{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac84775-4903-40bd-a838-c113796b9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shkwon\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:372: UserWarning: `build()` was called on layer 'sentiment_analysis_model', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sentiment_analysis_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sentiment_analysis_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def clean_logs(data_dir):\n",
    "    logs_dir = os.path.join(data_dir, \"logs\")\n",
    "    shutil.rmtree(logs_dir, ignore_errors=True)\n",
    "    return logs_dir\n",
    "\n",
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    local_file = local_file.replace(\"%20\", \" \")\n",
    "    p = tf.keras.utils.get_file(local_file, url, \n",
    "        extract=True, cache_dir=\".\")\n",
    "    local_folder = os.path.join(\"datasets\", local_file.split('.')[0])\n",
    "    labeled_sentences = []\n",
    "    for labeled_filename in os.listdir(local_folder):\n",
    "        if labeled_filename.endswith(\"_labelled.txt\"):\n",
    "            with open(os.path.join(local_folder, labeled_filename), \"r\") as f:\n",
    "                for line in f:\n",
    "                    sentence, label = line.strip().split('\\t')\n",
    "                    labeled_sentences.append((sentence, label))\n",
    "    return labeled_sentences\n",
    "\n",
    "class SentimentAnalysisModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_seqlen, **kwargs):\n",
    "        super(SentimentAnalysisModel, self).__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, max_seqlen)\n",
    "        self.bilstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(max_seqlen)\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# clean up log area\n",
    "data_dir = \"./data\"\n",
    "logs_dir = clean_logs(data_dir)\n",
    "\n",
    "# download and read data into data structures\n",
    "labeled_sentences = download_and_read(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\")\n",
    "sentences = [s for (s, l) in labeled_sentences]\n",
    "labels = [int(l) for (s, l) in labeled_sentences]\n",
    "\n",
    "# tokenize sentences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_counts)\n",
    "print(\"vocabulary size: {:d}\".format(vocab_size))\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for (k, v) in word2idx.items()}\n",
    "\n",
    "# seq_lengths = np.array([len(s.split()) for s in sentences])\n",
    "# print([(p, np.percentile(seq_lengths, p)) for p \n",
    "#     in [75, 80, 90, 95, 99, 100]])\n",
    "# [(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n",
    "max_seqlen = 64\n",
    "\n",
    "# create dataset\n",
    "sentences_as_ints = tokenizer.texts_to_sequences(sentences)\n",
    "sentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sentences_as_ints, maxlen=max_seqlen)\n",
    "labels_as_ints = np.array(labels)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sentences_as_ints, labels_as_ints))\n",
    "\n",
    "# split into train and test\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sentences) // 3\n",
    "val_size = (len(sentences) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "# define model\n",
    "# vocab_size + 1 to account for PAD character\n",
    "model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "model.summary()\n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b321dd05-7f96-4a3d-989b-5874945065a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.5106 - loss: 0.6928 - val_accuracy: 0.7450 - val_loss: 0.6732\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6808 - loss: 0.6393 - val_accuracy: 0.8050 - val_loss: 0.3967\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7935 - loss: 0.4489 - val_accuracy: 0.9150 - val_loss: 0.2643\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9090 - loss: 0.2716 - val_accuracy: 0.9600 - val_loss: 0.1221\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9351 - loss: 0.1981 - val_accuracy: 0.9600 - val_loss: 0.1290\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9634 - loss: 0.1391 - val_accuracy: 0.9750 - val_loss: 0.1031\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9654 - loss: 0.1022 - val_accuracy: 0.9800 - val_loss: 0.0744\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9792 - loss: 0.0750 - val_accuracy: 0.9950 - val_loss: 0.0318\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9889 - loss: 0.0394 - val_accuracy: 0.9850 - val_loss: 0.0274\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9910 - loss: 0.0362 - val_accuracy: 0.9900 - val_loss: 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shkwon\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:372: UserWarning: `build()` was called on layer 'sentiment_analysis_model_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4616 - loss: 0.6936\n",
      "test loss: 0.693, test accuracy: 0.491\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step \n",
      "0\t0\twho in their right mind is gonna buy this battery\n",
      "0\t0\tthe one big drawback of the mp3 player is that the buttons on the phone's front cover that let you pause and skip songs lock out after a few seconds\n",
      "0\t0\tnot easy to watch\n",
      "1\t0\tif you have not seen this movie i definitely recommend it\n",
      "1\t0\tthis place is two thumbs up way up\n",
      "1\t0\tpersonally i think it shows that people should learn to find a compromise them self without involving other people into issue\n",
      "1\t0\tthe block was amazing\n",
      "0\t0\tthis one just fails to create any real suspense\n",
      "0\t1\tdoesn't work\n",
      "0\t0\twhy are these sad little vegetables so overcooked\n",
      "0\t0\twe won't be going back\n",
      "1\t0\twill definitely be back\n",
      "1\t0\ti advise you to look out for it\n",
      "0\t0\teverything stinks\n",
      "0\t0\tsadly gordon ramsey's steak is a place we shall sharply avoid during our next trip to vegas\n",
      "0\t0\ti don't like this nokia either\n",
      "0\t0\ti wouldn't recommend buying this product\n",
      "0\t0\ti was shocked because no signs indicate cash only\n",
      "0\t0\tall in all i'd expected a better consumer experience from motorola\n",
      "0\t0\twe won't be returning\n",
      "0\t0\tvery much disappointed with this company\n",
      "0\t0\tafter a year the battery went completely dead on my headset\n",
      "1\t0\tfantastic food\n",
      "1\t0\tgreat time family dinner on a sunday night\n",
      "0\t0\ti am not impressed with this and i would not recommend this item to anyone\n",
      "1\t1\tsweetest phone\n",
      "1\t0\tnothing short of magnificent photography cinematography in this film\n",
      "1\t0\teveryone is very attentive providing excellent customer service\n",
      "0\t0\tdo not waste your time\n",
      "0\t0\tartless camera work endlessly presents us with the ugliest setting imaginable i e\n",
      "0\t0\tif it were possible to give them zero stars they'd have it\n",
      "0\t0\tif you check the director's filmography on this site you will see why this film didn't have a chance\n",
      "0\t0\the can't act one of the least scary villains i have ever seen he can't write did he write this damn movie in his sleep\n",
      "0\t0\tit shouldn't take 30 min for pancakes and eggs\n",
      "1\t0\tthey have a really nice atmosphere\n",
      "0\t0\tjust whatever you do avoid groove as its the antithesis of all that is good about human traffic\n",
      "0\t0\ti did not bother contacting the company for few dollar product but i learned the lesson that i should not have bought this form online anyway\n",
      "0\t0\ti don't recommend unless your car breaks down in front of it and you are starving\n",
      "0\t0\tthen there's the plot holes\n",
      "0\t0\tnever again will i be dining at this place\n",
      "1\t0\tgreat subway in fact it's so good when you come here every other subway will not meet your expectations\n",
      "1\t0\ttheir on screen chemistry critical to the entire film is genuine\n",
      "1\t0\ti personally love the hummus pita baklava falafels and baba ganoush it's amazing what they do with eggplant\n",
      "0\t0\tconsidering the relations off screen between taylor and stanwyck it was surprising how little chemistry there was on screen between the two of them\n",
      "0\t0\ti don't think i'll be running back to carly's anytime soon for food\n",
      "0\t0\tpiece of trash\n",
      "0\t0\tthis allows the possibility of double booking for the same date and time after the first\n",
      "0\t0\tit was a drive to get there\n",
      "1\t0\tdef coming back to bowl next time\n",
      "0\t0\tthe kids play area is nasty\n",
      "0\t0\tbased on the sub par service i received and no effort to show their gratitude for my business i won't be going back\n",
      "0\t0\tstar trek v the final frontier is the worst in the series\n",
      "1\t0\tyou won't regret it\n",
      "0\t0\tspend your money and time some place else\n",
      "1\t0\tseriously flavorful delights folks\n",
      "0\t0\twas not happy\n",
      "1\t0\ti loved their mussels cooked in this wine reduction the duck was tender and their potato dishes were delicious\n",
      "1\t0\tthe inside is really quite nice and very clean\n",
      "1\t0\tseller shipped quickly and much cheaper than the competitors\n",
      "1\t0\tloved it friendly servers great food wonderful and imaginative menu\n",
      "1\t0\tmagical help\n",
      "0\t0\tnot as good as i had hoped\n",
      "0\t0\tnobody identifies with these characters because they're all cardboard cutouts and stereotypes or predictably reverse stereotypes\n",
      "0\t0\tand sometimes it was so embarrassing that i had to look away\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step\n",
      "accuracy score: 0.509\n",
      "confusion matrix\n",
      "[[470  47]\n",
      " [444  39]]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "best_model_file = os.path.join(data_dir, \"best_model.weights.h5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "num_epochs = 10\n",
    "history = model.fit(train_dataset, epochs=num_epochs, \n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, tensorboard])\n",
    "\n",
    "# evaluate with test set\n",
    "best_model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\n",
    "best_model.build(input_shape=(batch_size, max_seqlen))\n",
    "best_model.load_weights(best_model_file)\n",
    "best_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
    "print(\"test loss: {:.3f}, test accuracy: {:.3f}\".format(test_loss, test_acc))\n",
    "\n",
    "# predict on batches\n",
    "labels, predictions = [], []\n",
    "idx2word[0] = \"PAD\"\n",
    "is_first_batch = True\n",
    "for test_batch in test_dataset:\n",
    "    inputs_b, labels_b = test_batch\n",
    "    pred_batch = best_model.predict(inputs_b)\n",
    "    predictions.extend([(1 if p > 0.5 else 0) for p in pred_batch])\n",
    "    labels.extend([l for l in labels_b])\n",
    "    if is_first_batch:\n",
    "        for rid in range(inputs_b.shape[0]):\n",
    "            words = [idx2word[idx] for idx in inputs_b[rid].numpy()]\n",
    "            words = [w for w in words if w != \"PAD\"]\n",
    "            sentence = \" \".join(words)\n",
    "            print(\"{:d}\\t{:d}\\t{:s}\".format(labels[rid], predictions[rid], sentence))\n",
    "        is_first_batch = False\n",
    "\n",
    "print(\"accuracy score: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de0b5d-8200-4df4-997f-73dea30bc04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
