{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89db736-cbe3-4bd6-83d1-7ca13321e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "#import nltk\n",
    "#nltk.download(\"treebank\")\n",
    "#nltk.download()\n",
    "\n",
    "def clean_logs(data_dir):\n",
    "    logs_dir = os.path.join(data_dir, \"logs\")\n",
    "    shutil.rmtree(logs_dir, ignore_errors=True)\n",
    "    return logs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2f16a2-1293-448e-84c6-791f076f9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(dataset_dir, num_pairs=None):\n",
    "    sent_filename = os.path.join(dataset_dir, \"treebank-sents.txt\")\n",
    "    poss_filename = os.path.join(dataset_dir, \"treebank-poss.txt\")\n",
    "    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\n",
    "        import nltk    \n",
    "        if not os.path.exists(dataset_dir):\n",
    "            os.makedirs(dataset_dir)\n",
    "        fsents = open(sent_filename, \"w\")\n",
    "        fposs = open(poss_filename, \"w\")\n",
    "        sentences = nltk.corpus.treebank.tagged_sents()\n",
    "        print(sentences)\n",
    "        for sent in sentences:\n",
    "            fsents.write(\" \".join([w for w, p in sent]) + \"\\n\")\n",
    "            fposs.write(\" \".join([p for w, p in sent]) + \"\\n\")\n",
    "        fsents.close()\n",
    "        fposs.close()\n",
    "    sents, poss = [], []\n",
    "    with open(sent_filename, \"r\") as fsent:\n",
    "        for idx, line in enumerate(fsent):\n",
    "            sents.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs:\n",
    "                break\n",
    "    with open(poss_filename, \"r\") as fposs:\n",
    "        for idx, line in enumerate(fposs):\n",
    "            poss.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs:\n",
    "                break\n",
    "    return sents, poss\n",
    "\n",
    "def tokenize_and_build_vocab(texts, vocab_size=None, lower=True):\n",
    "    if vocab_size is None:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n",
    "    else:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size+1, oov_token=\"UNK\", lower=lower)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    if vocab_size is not None:\n",
    "        # additional workaround, see issue 8092\n",
    "        # https://github.com/keras-team/keras/issues/8092\n",
    "        tokenizer.word_index = {e:i for e, i in tokenizer.word_index.items() \n",
    "            if i <= vocab_size+1 }\n",
    "    word2idx = tokenizer.word_index\n",
    "    idx2word = {v:k for k, v in word2idx.items()}\n",
    "    return word2idx, idx2word, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf02b30e-b220-480b-a3d7-3f37982d7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTaggingModel(tf.keras.Model):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size,\n",
    "            embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n",
    "        super(POSTaggingModel, self).__init__(**kwargs)\n",
    "        self.embed = tf.keras.layers.Embedding(\n",
    "            source_vocab_size, embedding_dim, input_length=max_seqlen)\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))\n",
    "        self.dense = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(target_vocab_size))\n",
    "        self.activation = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.rnn(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "def masked_accuracy():\n",
    "    def masked_accuracy_fn(ytrue, ypred):\n",
    "        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n",
    "        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n",
    " \n",
    "        mask = tf.keras.backend.cast(\n",
    "            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n",
    "        matches = tf.keras.backend.cast(\n",
    "            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n",
    "        numer = tf.keras.backend.sum(matches)\n",
    "        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n",
    "        accuracy =  numer / denom\n",
    "        return accuracy\n",
    "\n",
    "    return masked_accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0378de20-142e-4f87-9a25-5bfcff533d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records: 3914\n"
     ]
    }
   ],
   "source": [
    "NUM_PAIRS = None\n",
    "EMBEDDING_DIM = 128\n",
    "RNN_OUTPUT_DIM = 256\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# clean up log area\n",
    "data_dir = \"./data\"\n",
    "logs_dir = clean_logs(data_dir)\n",
    "\n",
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "sents, poss = download_and_read(\"./datasets\")\n",
    "assert(len(sents) == len(poss))\n",
    "print(\"# of records: {:d}\".format(len(sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ff3064-7cfa-4530-8d70-ac310b590de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab sizes (source): 9001, (target): 39\n",
      "[(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]\n"
     ]
    }
   ],
   "source": [
    "word2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(sents, vocab_size=9000)\n",
    "word2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(poss, vocab_size=38, lower=False)\n",
    "source_vocab_size = len(word2idx_s)\n",
    "target_vocab_size = len(word2idx_t)\n",
    "print(\"vocab sizes (source): {:d}, (target): {:d}\".format(source_vocab_size, target_vocab_size))\n",
    "\n",
    "sequence_lengths = np.array([len(s.split()) for s in sents])\n",
    "print([(p, np.percentile(sequence_lengths, p)) for p in [75, 80, 90, 95, 99, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "392a1459-415e-4e91-bc35-e733e9256801",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 271\n",
    "\n",
    "# create dataset\n",
    "sents_as_ints = tokenizer_s.texts_to_sequences(sents)\n",
    "sents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sents_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
    "poss_as_ints = tokenizer_t.texts_to_sequences(poss)\n",
    "poss_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sents_as_ints, poss_as_ints))\n",
    "idx2word_s[0], idx2word_t[0] = \"PAD\", \"PAD\"\n",
    "poss_as_catints = []\n",
    "for p in poss_as_ints:\n",
    "    poss_as_catints.append(tf.keras.utils.to_categorical(p, num_classes=target_vocab_size))\n",
    "poss_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_catints, maxlen=max_seqlen)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sents_as_ints, poss_as_catints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6eae78-b871-44b7-870d-f1706725231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training, validation, and test datasets\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sents) // 3\n",
    "val_size = (len(sents) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "# create batches\n",
    "batch_size = BATCH_SIZE\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5892d8df-ac68-4abf-8c7c-69d3260a3a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pos_tagging_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1152128   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr multiple                  0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional multiple                  592896    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri multiple                  20007     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      multiple                  0         \n",
      "=================================================================\n",
      "Total params: 1,765,031\n",
      "Trainable params: 1,765,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "embedding_dim = EMBEDDING_DIM\n",
    "rnn_output_dim = RNN_OUTPUT_DIM\n",
    "\n",
    "model = POSTaggingModel(source_vocab_size, target_vocab_size,\n",
    "    embedding_dim, max_seqlen, rnn_output_dim)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\", masked_accuracy()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb7a14-f20d-46a1-88cc-5266a1abe35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 14s 276ms/step - loss: 1.4765 - accuracy: 0.8654 - masked_accuracy_fn: 0.0014 - val_loss: 0.3119 - val_accuracy: 0.9168 - val_masked_accuracy_fn: 0.5317\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.3283 - accuracy: 0.9195 - masked_accuracy_fn: 0.1198 - val_loss: 0.3221 - val_accuracy: 0.9237 - val_masked_accuracy_fn: 0.1308\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 2s 85ms/step - loss: 0.3173 - accuracy: 0.9235 - masked_accuracy_fn: 0.1684 - val_loss: 0.3278 - val_accuracy: 0.9166 - val_masked_accuracy_fn: 0.2105\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.2971 - accuracy: 0.9192 - masked_accuracy_fn: 0.1452 - val_loss: 0.2961 - val_accuracy: 0.9144 - val_masked_accuracy_fn: 0.0911\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 2s 85ms/step - loss: 0.2684 - accuracy: 0.9229 - masked_accuracy_fn: 0.1290 - val_loss: 0.2527 - val_accuracy: 0.9263 - val_masked_accuracy_fn: 0.1351\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.2538 - accuracy: 0.9274 - masked_accuracy_fn: 0.1572 - val_loss: 0.2563 - val_accuracy: 0.9247 - val_masked_accuracy_fn: 0.1447\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.2496 - accuracy: 0.9265 - masked_accuracy_fn: 0.1499 - val_loss: 0.2427 - val_accuracy: 0.9305 - val_masked_accuracy_fn: 0.1906\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.2424 - accuracy: 0.9306 - masked_accuracy_fn: 0.1922 - val_loss: 0.2382 - val_accuracy: 0.9325 - val_masked_accuracy_fn: 0.2269\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.2370 - accuracy: 0.9345 - masked_accuracy_fn: 0.2401 - val_loss: 0.2390 - val_accuracy: 0.9346 - val_masked_accuracy_fn: 0.2464\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.2320 - accuracy: 0.9371 - masked_accuracy_fn: 0.2842 - val_loss: 0.2280 - val_accuracy: 0.9385 - val_masked_accuracy_fn: 0.3292\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.2197 - accuracy: 0.9405 - masked_accuracy_fn: 0.3168 - val_loss: 0.1997 - val_accuracy: 0.9465 - val_masked_accuracy_fn: 0.3576\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.2086 - accuracy: 0.9433 - masked_accuracy_fn: 0.3454 - val_loss: 0.2112 - val_accuracy: 0.9420 - val_masked_accuracy_fn: 0.3621\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.2013 - accuracy: 0.9448 - masked_accuracy_fn: 0.3692 - val_loss: 0.1919 - val_accuracy: 0.9476 - val_masked_accuracy_fn: 0.4063\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.1945 - accuracy: 0.9467 - masked_accuracy_fn: 0.3979 - val_loss: 0.1843 - val_accuracy: 0.9497 - val_masked_accuracy_fn: 0.4051\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.1855 - accuracy: 0.9499 - masked_accuracy_fn: 0.4328 - val_loss: 0.1790 - val_accuracy: 0.9521 - val_masked_accuracy_fn: 0.4773\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.1783 - accuracy: 0.9525 - masked_accuracy_fn: 0.4620 - val_loss: 0.1660 - val_accuracy: 0.9564 - val_masked_accuracy_fn: 0.4848\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.1712 - accuracy: 0.9549 - masked_accuracy_fn: 0.4909 - val_loss: 0.1716 - val_accuracy: 0.9549 - val_masked_accuracy_fn: 0.5464\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.1626 - accuracy: 0.9572 - masked_accuracy_fn: 0.5131 - val_loss: 0.1540 - val_accuracy: 0.9595 - val_masked_accuracy_fn: 0.5659\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.1583 - accuracy: 0.9586 - masked_accuracy_fn: 0.5385 - val_loss: 0.1411 - val_accuracy: 0.9625 - val_masked_accuracy_fn: 0.5802\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.1494 - accuracy: 0.9607 - masked_accuracy_fn: 0.5542 - val_loss: 0.1403 - val_accuracy: 0.9632 - val_masked_accuracy_fn: 0.6124\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.1418 - accuracy: 0.9624 - masked_accuracy_fn: 0.5688 - val_loss: 0.1426 - val_accuracy: 0.9618 - val_masked_accuracy_fn: 0.5882\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.1407 - accuracy: 0.9624 - masked_accuracy_fn: 0.5759 - val_loss: 0.1264 - val_accuracy: 0.9666 - val_masked_accuracy_fn: 0.6580\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.1311 - accuracy: 0.9648 - masked_accuracy_fn: 0.5974 - val_loss: 0.1361 - val_accuracy: 0.9625 - val_masked_accuracy_fn: 0.6008\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.1264 - accuracy: 0.9654 - masked_accuracy_fn: 0.6039 - val_loss: 0.1206 - val_accuracy: 0.9661 - val_masked_accuracy_fn: 0.5741\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.1241 - accuracy: 0.9654 - masked_accuracy_fn: 0.6068 - val_loss: 0.1243 - val_accuracy: 0.9643 - val_masked_accuracy_fn: 0.5901\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.1205 - accuracy: 0.9659 - masked_accuracy_fn: 0.6146 - val_loss: 0.1097 - val_accuracy: 0.9681 - val_masked_accuracy_fn: 0.6902\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 0.1157 - accuracy: 0.9670 - masked_accuracy_fn: 0.6256 - val_loss: 0.1091 - val_accuracy: 0.9685 - val_masked_accuracy_fn: 0.6540\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.1155 - accuracy: 0.9670 - masked_accuracy_fn: 0.6281 - val_loss: 0.1218 - val_accuracy: 0.9643 - val_masked_accuracy_fn: 0.6205\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.1073 - accuracy: 0.9692 - masked_accuracy_fn: 0.6447 - val_loss: 0.1018 - val_accuracy: 0.9707 - val_masked_accuracy_fn: 0.7100\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.1044 - accuracy: 0.9699 - masked_accuracy_fn: 0.6573 - val_loss: 0.0977 - val_accuracy: 0.9712 - val_masked_accuracy_fn: 0.7093\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.1042 - accuracy: 0.9698 - masked_accuracy_fn: 0.6554 - val_loss: 0.0928 - val_accuracy: 0.9727 - val_masked_accuracy_fn: 0.6472\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.1025 - accuracy: 0.9702 - masked_accuracy_fn: 0.6652"
     ]
    }
   ],
   "source": [
    "# train\n",
    "num_epochs = NUM_EPOCHS\n",
    "\n",
    "best_model_file = os.path.join(data_dir, \"best_model.weights.h5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    best_model_file, \n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "history = model.fit(train_dataset, \n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e1107-26b7-442a-880e-e5693bf0e937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
