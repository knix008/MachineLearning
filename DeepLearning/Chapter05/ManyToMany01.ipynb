{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d89db736-cbe3-4bd6-83d1-7ca13321e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "#import nltk\n",
    "#nltk.download(\"treebank\")\n",
    "#nltk.download()\n",
    "\n",
    "def clean_logs(data_dir):\n",
    "    logs_dir = os.path.join(data_dir, \"logs\")\n",
    "    shutil.rmtree(logs_dir, ignore_errors=True)\n",
    "    return logs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af2f16a2-1293-448e-84c6-791f076f9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(dataset_dir, num_pairs=None):\n",
    "    sent_filename = os.path.join(dataset_dir, \"treebank-sents.txt\")\n",
    "    poss_filename = os.path.join(dataset_dir, \"treebank-poss.txt\")\n",
    "    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\n",
    "        import nltk    \n",
    "        if not os.path.exists(dataset_dir):\n",
    "            os.makedirs(dataset_dir)\n",
    "        fsents = open(sent_filename, \"w\")\n",
    "        fposs = open(poss_filename, \"w\")\n",
    "        sentences = nltk.corpus.treebank.tagged_sents()\n",
    "        print(sentences)\n",
    "        for sent in sentences:\n",
    "            fsents.write(\" \".join([w for w, p in sent]) + \"\\n\")\n",
    "            fposs.write(\" \".join([p for w, p in sent]) + \"\\n\")\n",
    "        fsents.close()\n",
    "        fposs.close()\n",
    "    sents, poss = [], []\n",
    "    with open(sent_filename, \"r\") as fsent:\n",
    "        for idx, line in enumerate(fsent):\n",
    "            sents.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs:\n",
    "                break\n",
    "    with open(poss_filename, \"r\") as fposs:\n",
    "        for idx, line in enumerate(fposs):\n",
    "            poss.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs:\n",
    "                break\n",
    "    return sents, poss\n",
    "\n",
    "def tokenize_and_build_vocab(texts, vocab_size=None, lower=True):\n",
    "    if vocab_size is None:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n",
    "    else:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size+1, oov_token=\"UNK\", lower=lower)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    if vocab_size is not None:\n",
    "        # additional workaround, see issue 8092\n",
    "        # https://github.com/keras-team/keras/issues/8092\n",
    "        tokenizer.word_index = {e:i for e, i in tokenizer.word_index.items() \n",
    "            if i <= vocab_size+1 }\n",
    "    word2idx = tokenizer.word_index\n",
    "    idx2word = {v:k for k, v in word2idx.items()}\n",
    "    return word2idx, idx2word, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf02b30e-b220-480b-a3d7-3f37982d7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTaggingModel(tf.keras.Model):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size,\n",
    "            embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n",
    "        super(POSTaggingModel, self).__init__(**kwargs)\n",
    "        self.embed = tf.keras.layers.Embedding(\n",
    "            source_vocab_size, embedding_dim, input_length=max_seqlen)\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))\n",
    "        self.dense = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(target_vocab_size))\n",
    "        self.activation = tf.keras.layers.Activation(\"softmax\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.rnn(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "def masked_accuracy():\n",
    "    def masked_accuracy_fn(ytrue, ypred):\n",
    "        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n",
    "        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n",
    " \n",
    "        mask = tf.keras.backend.cast(\n",
    "            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n",
    "        matches = tf.keras.backend.cast(\n",
    "            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n",
    "        numer = tf.keras.backend.sum(matches)\n",
    "        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n",
    "        accuracy =  numer / denom\n",
    "        return accuracy\n",
    "\n",
    "    return masked_accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0378de20-142e-4f87-9a25-5bfcff533d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records: 3914\n"
     ]
    }
   ],
   "source": [
    "NUM_PAIRS = None\n",
    "EMBEDDING_DIM = 128\n",
    "RNN_OUTPUT_DIM = 256\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# clean up log area\n",
    "data_dir = \"./data\"\n",
    "logs_dir = clean_logs(data_dir)\n",
    "\n",
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "sents, poss = download_and_read(\"./datasets\")\n",
    "assert(len(sents) == len(poss))\n",
    "print(\"# of records: {:d}\".format(len(sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79ff3064-7cfa-4530-8d70-ac310b590de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab sizes (source): 9001, (target): 39\n",
      "[(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]\n"
     ]
    }
   ],
   "source": [
    "word2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(sents, vocab_size=9000)\n",
    "word2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(poss, vocab_size=38, lower=False)\n",
    "source_vocab_size = len(word2idx_s)\n",
    "target_vocab_size = len(word2idx_t)\n",
    "print(\"vocab sizes (source): {:d}, (target): {:d}\".format(source_vocab_size, target_vocab_size))\n",
    "\n",
    "sequence_lengths = np.array([len(s.split()) for s in sents])\n",
    "print([(p, np.percentile(sequence_lengths, p)) for p in [75, 80, 90, 95, 99, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "392a1459-415e-4e91-bc35-e733e9256801",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 271\n",
    "\n",
    "# create dataset\n",
    "sents_as_ints = tokenizer_s.texts_to_sequences(sents)\n",
    "sents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sents_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
    "poss_as_ints = tokenizer_t.texts_to_sequences(poss)\n",
    "poss_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sents_as_ints, poss_as_ints))\n",
    "idx2word_s[0], idx2word_t[0] = \"PAD\", \"PAD\"\n",
    "poss_as_catints = []\n",
    "for p in poss_as_ints:\n",
    "    poss_as_catints.append(tf.keras.utils.to_categorical(p, num_classes=target_vocab_size))\n",
    "poss_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_catints, maxlen=max_seqlen)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sents_as_ints, poss_as_catints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca6eae78-b871-44b7-870d-f1706725231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training, validation, and test datasets\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sents) // 3\n",
    "val_size = (len(sents) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "# create batches\n",
    "batch_size = BATCH_SIZE\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5892d8df-ac68-4abf-8c7c-69d3260a3a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shkwon\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\layer.py:361: UserWarning: `build()` was called on layer 'pos_tagging_model_4', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pos_tagging_model_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"pos_tagging_model_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_4             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d_4             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define model\n",
    "embedding_dim = EMBEDDING_DIM\n",
    "rnn_output_dim = RNN_OUTPUT_DIM\n",
    "\n",
    "model = POSTaggingModel(source_vocab_size, target_vocab_size,\n",
    "    embedding_dim, max_seqlen, rnn_output_dim)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\", masked_accuracy()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23fb7a14-f20d-46a1-88cc-5266a1abe35c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m NUM_EPOCHS\n\u001b[1;32m----> 4\u001b[0m best_model_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m      6\u001b[0m     best_model_file, \n\u001b[0;32m      7\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m tensorboard \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlogs_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# train\n",
    "num_epochs = NUM_EPOCHS\n",
    "\n",
    "best_model_file = os.path.join(data_dir, \"best_model.weights.h5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    best_model_file, \n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "history = model.fit(train_dataset, \n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e1107-26b7-442a-880e-e5693bf0e937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
