import torch
from diffusers import Flux2Pipeline, Flux2PriorReduxPipeline, Flux2Img2ImgPipeline
from datetime import datetime
from PIL import Image
import os
import warnings
import gradio as gr
import numpy as np

# Suppress warnings
warnings.filterwarnings("ignore", message=".*add_prefix_spade.*")
warnings.filterwarnings("ignore", message=".*add_prefix_space.*")
warnings.filterwarnings("ignore", message=".*slow tokenizers.*")

# Set device and data type
device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32

print(f"Device: {device}, dtype: {dtype}")
print("ëª¨ë¸ ë¡œë”© ì¤‘...")

# Load FLUX.2 Redux pipeline for multi-image input
pipe_redux = Flux2PriorReduxPipeline.from_pretrained(
    "black-forest-labs/FLUX.2-Redux-dev", torch_dtype=dtype
).to(device)

# Load base FLUX.2 pipeline
pipe_base = Flux2Pipeline.from_pretrained(
    "black-forest-labs/FLUX.2-dev",
    text_encoder=None,
    text_encoder_2=None,
    torch_dtype=dtype
).to(device)

# Load Img2Img pipeline for image-to-image generation
pipe_img2img = Flux2Img2ImgPipeline.from_pretrained(
    "black-forest-labs/FLUX.2-dev", torch_dtype=dtype
).to(device)

# Enable memory optimizations
if device == "cpu":
    pipe_base.enable_model_cpu_offload()
    pipe_base.enable_attention_slicing(1)
    pipe_img2img.enable_model_cpu_offload()
    pipe_img2img.enable_attention_slicing(1)
else:
    pipe_base.enable_model_cpu_offload()
    pipe_img2img.enable_model_cpu_offload()

print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")


def resize_image_to_multiple_of_64(image, target_width=None, target_height=None):
    """
    Resize image so dimensions are multiples of 64
    
    Parameters:
    -----------
    image : PIL.Image
        Input image
    target_width : int
        Target width (if None, use image width)
    target_height : int
        Target height (if None, use image height)
    
    Returns:
    --------
    PIL.Image
        Resized image
    """
    width = target_width or image.width
    height = target_height or image.height
    
    # Round to nearest multiple of 64
    width = (width // 64) * 64
    height = (height // 64) * 64
    
    # Ensure minimum size
    width = max(width, 256)
    height = max(height, 256)
    
    return image.resize((width, height), Image.Resampling.LANCZOS)


def generate_from_multi_images(
    image1, image2, image3, image4,
    width, height, guidance_scale, num_inference_steps, seed,
    mode, strength
):
    """
    ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Parameters:
    -----------
    image1-4 : PIL.Image
        ì…ë ¥ ì´ë¯¸ì§€ë“¤ (ìµœì†Œ 1ê°œ í•„ìš”)
    width, height : int
        ì¶œë ¥ ì´ë¯¸ì§€ í¬ê¸°
    guidance_scale : float
        í”„ë¡¬í”„íŠ¸ ê°•ë„
    num_inference_steps : int
        ì¶”ë¡  ìŠ¤í… ìˆ˜
    seed : int
        ëœë¤ ì‹œë“œ
    mode : str
        ìƒì„± ëª¨ë“œ ("redux" ë˜ëŠ” "img2img")
    strength : float
        ì´ë¯¸ì§€ ë³€í™˜ ê°•ë„
    """
    try:
        # Collect valid images
        images = [img for img in [image1, image2, image3, image4] if img is not None]

        if len(images) == 0:
            return None, "âœ— ì˜¤ë¥˜: ìµœì†Œ 1ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”"

        print(f"ì…ë ¥ ì´ë¯¸ì§€ ìˆ˜: {len(images)}")

        # Ensure dimensions are multiples of 64
        width = int(max((width // 64) * 64, 256))
        height = int(max((height // 64) * 64, 256))

        print(f"ìƒì„± ì„¤ì •: {width}x{height}, guidance={guidance_scale}, steps={num_inference_steps}")

        generator = torch.Generator(device=device).manual_seed(int(seed))

        if mode == "redux":
            # FLUX Redux ëª¨ë“œ: ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ ê²°í•©
            print("Redux ëª¨ë“œ: ì´ë¯¸ì§€ íŠ¹ì§• ê²°í•© ì¤‘...")

            # Process images through Redux pipeline
            # Redux can accept multiple images and blend their features
            redux_output = pipe_redux(images)

            # Generate final image using the combined embeddings
            image = pipe_base(
                prompt_embeds=redux_output.prompt_embeds,
                pooled_prompt_embeds=redux_output.pooled_prompt_embeds,
                width=width,
                height=height,
                guidance_scale=guidance_scale,
                num_inference_steps=int(num_inference_steps),
                generator=generator,
            ).images[0]

        else:
            # Img2Img ëª¨ë“œ: ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³€í™˜
            print("Img2Img ëª¨ë“œ: ì´ë¯¸ì§€ ë³€í™˜ ì¤‘...")

            # Use first image as base
            base_image = images[0].convert("RGB")
            base_image = resize_image_to_multiple_of_64(base_image, width, height)

            # If multiple images, blend them together as base
            if len(images) > 1:
                print(f"{len(images)}ê°œ ì´ë¯¸ì§€ ë¸”ë Œë”© ì¤‘...")
                blended = blend_images(images, width, height)
                base_image = blended

            # Get prompt from Redux if available
            redux_output = pipe_redux(images[0])

            image = pipe_img2img(
                image=base_image,
                prompt_embeds=redux_output.prompt_embeds,
                pooled_prompt_embeds=redux_output.pooled_prompt_embeds,
                width=width,
                height=height,
                strength=strength,
                guidance_scale=guidance_scale,
                num_inference_steps=int(num_inference_steps),
                generator=generator,
            ).images[0]

        # Save with timestamp
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        script_name = os.path.splitext(os.path.basename(__file__))[0]
        filename = f"{script_name}_{timestamp}.png"
        image.save(filename)

        return image, f"âœ“ ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}"

    except Exception as e:
        import traceback
        error_msg = f"âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(traceback.format_exc())
        return None, error_msg


def blend_images(images, width, height):
    """ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ë¸”ë Œë”©í•˜ì—¬ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ ë§Œë“­ë‹ˆë‹¤."""
    # Resize all images to target size
    resized = []
    for img in images:
        img_rgb = img.convert("RGB")
        img_resized = img_rgb.resize((width, height), Image.Resampling.LANCZOS)
        resized.append(np.array(img_resized, dtype=np.float32))

    # Average blend
    blended = np.mean(resized, axis=0).astype(np.uint8)
    return Image.fromarray(blended)


# Create Gradio interface
with gr.Blocks(title="Flux.1-dev Multi-Image Editor") as interface:
    gr.Markdown("# ğŸ¨ Flux.1-dev ë©€í‹° ì´ë¯¸ì§€ ì—ë””í„°")
    gr.Markdown("ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„ FLUX Reduxë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    with gr.Row():
        with gr.Column(scale=1):
            # Input images - 4ê°œê¹Œì§€ ì§€ì›
            gr.Markdown("### ì…ë ¥ ì´ë¯¸ì§€ (ìµœëŒ€ 4ê°œ)")
            with gr.Row():
                image1 = gr.Image(
                    label="ì´ë¯¸ì§€ 1 (í•„ìˆ˜)",
                    type="pil",
                    sources=["upload"],
                    height=200,
                )
                image2 = gr.Image(
                    label="ì´ë¯¸ì§€ 2 (ì„ íƒ)",
                    type="pil",
                    sources=["upload"],
                    height=200,
                )
            with gr.Row():
                image3 = gr.Image(
                    label="ì´ë¯¸ì§€ 3 (ì„ íƒ)",
                    type="pil",
                    sources=["upload"],
                    height=200,
                )
                image4 = gr.Image(
                    label="ì´ë¯¸ì§€ 4 (ì„ íƒ)",
                    type="pil",
                    sources=["upload"],
                    height=200,
                )

            # Generation mode
            mode = gr.Radio(
                label="ìƒì„± ëª¨ë“œ",
                choices=["redux", "img2img"],
                value="redux",
                info="redux: ì—¬ëŸ¬ ì´ë¯¸ì§€ íŠ¹ì§• ê²°í•© / img2img: ì²« ì´ë¯¸ì§€ ê¸°ë°˜ ë³€í™˜",
            )

            with gr.Row():
                width = gr.Number(
                    label="ì´ë¯¸ì§€ ë„ˆë¹„",
                    value=512,
                    step=64,
                    precision=0,
                    info="ìƒì„±í•  ì´ë¯¸ì§€ì˜ ë„ˆë¹„ (í”½ì…€). 64ì˜ ë°°ìˆ˜.",
                )
                height = gr.Number(
                    label="ì´ë¯¸ì§€ ë†’ì´",
                    value=768,
                    step=64,
                    precision=0,
                    info="ìƒì„±í•  ì´ë¯¸ì§€ì˜ ë†’ì´ (í”½ì…€). 64ì˜ ë°°ìˆ˜.",
                )

            with gr.Row():
                guidance_scale = gr.Slider(
                    label="Guidance Scale",
                    minimum=1.0,
                    maximum=20.0,
                    step=0.5,
                    value=4.0,
                    info="ì´ë¯¸ì§€ íŠ¹ì§•ì„ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ë”°ë¥¼ì§€ ì œì–´í•©ë‹ˆë‹¤.",
                )
                num_inference_steps = gr.Slider(
                    label="ì¶”ë¡  ìŠ¤í…",
                    minimum=10,
                    maximum=50,
                    step=1,
                    value=28,
                    info="ë†’ì„ìˆ˜ë¡ í’ˆì§ˆì´ ì¢‹ì§€ë§Œ ëŠë¦½ë‹ˆë‹¤.",
                )

            with gr.Row():
                seed = gr.Number(
                    label="ì‹œë“œ",
                    value=100,
                    precision=0,
                    info="ê°™ì€ ì‹œë“œ = ê°™ì€ ê²°ê³¼",
                )
                strength = gr.Slider(
                    label="ê°•ë„ (img2img ëª¨ë“œìš©)",
                    minimum=0.1,
                    maximum=1.0,
                    step=0.1,
                    value=0.8,
                    info="img2img ëª¨ë“œì—ì„œ ì›ë³¸ ì´ë¯¸ì§€ ë³€í™˜ ê°•ë„",
                )

            generate_btn = gr.Button("ğŸš€ ì´ë¯¸ì§€ ìƒì„±", variant="primary", size="lg")

        with gr.Column(scale=1):
            # Output
            output_image = gr.Image(label="ìƒì„±ëœ ì´ë¯¸ì§€", height=600)
            output_message = gr.Textbox(label="ìƒíƒœ", interactive=False)

    # Connect the generate button to the function
    generate_btn.click(
        fn=generate_from_multi_images,
        inputs=[
            image1, image2, image3, image4,
            width, height,
            guidance_scale, num_inference_steps, seed,
            mode, strength,
        ],
        outputs=[output_image, output_message],
    )

    gr.Markdown("---")
    gr.Markdown(
        """
    ### ì‚¬ìš© ë°©ë²•:

    **Redux ëª¨ë“œ** (ê¶Œì¥)
    - ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ê³¼ íŠ¹ì§•ì„ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
    - ì˜ˆ: ì¸ë¬¼ ì‚¬ì§„ + ë°°ê²½ ì‚¬ì§„ â†’ ê²°í•©ëœ ì´ë¯¸ì§€
    - ìµœëŒ€ 4ê°œ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ ë¸”ë Œë”©í•©ë‹ˆë‹¤

    **Img2Img ëª¨ë“œ**
    - ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤
    - ì—¬ëŸ¬ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ í‰ê·  ë¸”ë Œë”© í›„ ë³€í™˜
    - strength ê°’ìœ¼ë¡œ ë³€í™˜ ê°•ë„ ì¡°ì ˆ

    ### íŒŒë¼ë¯¸í„° ì„¤ëª…:

    **ì…ë ¥ ì´ë¯¸ì§€**
    - ìµœì†Œ 1ê°œ, ìµœëŒ€ 4ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    - ì´ë¯¸ì§€ 1ì€ í•„ìˆ˜ì…ë‹ˆë‹¤

    **Guidance Scale**
    - ì…ë ¥ ì´ë¯¸ì§€ íŠ¹ì§•ì„ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ë”°ë¥¼ì§€ ì œì–´í•©ë‹ˆë‹¤
    - ê¶Œì¥ê°’: 3-7

    **ì¶”ë¡  ìŠ¤í…**
    - ì´ë¯¸ì§€ ìƒì„± ë‹¨ê³„ ìˆ˜ì…ë‹ˆë‹¤
    - ê¶Œì¥ê°’: 20-28

    **ì‹œë“œ** (Seed)
    - ë‚œìˆ˜ ìƒì„±ì˜ ì‹œì‘ì ì…ë‹ˆë‹¤
    - ê°™ì€ ì‹œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤

    **ê°•ë„** (Strength)
    - img2img ëª¨ë“œì—ì„œ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì–¼ë§ˆë‚˜ ë³€í™˜í• ì§€ ì œì–´
    - ë‚®ì„ìˆ˜ë¡ ì›ë³¸ ìœ ì§€, ë†’ì„ìˆ˜ë¡ ë§ì´ ë³€í™˜
    """
    )

# Launch the interface
if __name__ == "__main__":
    interface.launch(inbrowser=True)
